from concurrent.futures import ProcessPoolExecutor, as_completed

import os
import sys
import math
import numpy as np  # ç”¨äºæ•°å€¼è®¡ç®—
import mindspore as ms
import argparse
import traceback
import matplotlib.pyplot as plt
plt.ioff()  # å…³é—­äº¤äº’æ¨¡å¼ï¼Œè‡ªåŠ¨å…³é—­å›¾ç‰‡
# ä» multilevel_common.py å¯¼å…¥å…±äº«å‡½æ•°
from multilevel_common import (
    divide_graph,
    smart_divide_graph_with_qubit_constraint,  # æ™ºèƒ½å­å›¾åˆ’åˆ†ï¼Œæ”¯æŒé‡å­æ¯”ç‰¹çº¦æŸ
    count_conflicts,
    plot_original_graph,
    plot_New_IDs_subgraphs,
    plot_Original_IDs_subgraphs,
    plot_New_IDs_colored_subgraphs,
    plot_Original_IDs_colored_subgraphs,
    get_subgraph_coloring,
    visualize_graph,
    handle_exception,
)
# ä»ä¸‰ä¸ªä¸“é—¨æ¨¡å—å¯¼å…¥å„è‡ªç‰¹æœ‰çš„å‡½æ•°
from multilevel_adapt_QAOA_k_coloring import (
    sequential_process_subgraphs,  # é¡ºåºå¤„ç†å­å›¾ç€è‰²
    iterative_optimization,  # è¿­ä»£ä¼˜åŒ–ç€è‰²æ–¹æ¡ˆ
)
from multilevel_standard_QAOA_k_coloring import (
    sequential_process_subgraphs_standard,  # é¡ºåºå¤„ç†å­å›¾ç€è‰²
    iterative_optimization_standard,  # è¿­ä»£ä¼˜åŒ–ç€è‰²æ–¹æ¡ˆ
)
from multilevel_adapt_noise_QAOA_k_coloring import (
    sequential_process_subgraphs_noise,  # é¡ºåºå¤„ç†å­å›¾ç€è‰²
    iterative_optimization_noise,  # è¿­ä»£ä¼˜åŒ–ç€è‰²æ–¹æ¡ˆ
)
from graph_loader import load_graphs_from_dir, read_col_file
import csv, os, time, traceback, json, logging

# æ·»åŠ ç»å…¸ç®—æ³•æ¨¡å—è·¯å¾„
CURRENT_FILE_DIR = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(CURRENT_FILE_DIR)  # HadaQAOA
sys.path.insert(0, PARENT_DIR)
sys.path.insert(0, os.path.join(PARENT_DIR, "classical_algorithms"))

from greedy import GreedyColoring
from Backtracking_and_Welch_Powell import process_graph_with_welch_powell


def run_greedy_coloring(graph, filename):
    """è¿è¡Œè´ªå¿ƒç®—æ³•è¿›è¡Œå›¾ç€è‰²"""
    import time
    start_time = time.perf_counter()
    try:
        greedy = GreedyColoring(graph)
        coloring, num_colors = greedy.execute()
        conflicts = count_conflicts(coloring, graph)
        exec_time = (time.perf_counter() - start_time) * 1000  # æ¯«ç§’
        return {
            'algorithm': 'Greedy',
            'filename': filename,
            'num_nodes': graph.number_of_nodes(),
            'num_edges': graph.number_of_edges(),
            'num_colors': num_colors,
            'conflicts': conflicts,
            'execution_time_ms': round(exec_time, 4),
            'is_valid': conflicts == 0,
            'coloring': coloring
        }
    except Exception as e:
        print(f"âŒ è´ªå¿ƒç®—æ³•æ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()
        return None


def run_welch_powell_coloring(graph, filename):
    """è¿è¡ŒWelch-Powellç®—æ³•è¿›è¡Œå›¾ç€è‰²"""
    try:
        result = process_graph_with_welch_powell(graph, filename)
        if result:
            result['algorithm'] = 'WelchPowell'
        return result
    except Exception as e:
        print(f"âŒ Welch-Powellç®—æ³•æ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()
        return None


def validate_graph_feasibility(graph, max_nodes_per_subgraph):
    """éªŒè¯å›¾çš„å¯å¤„ç†æ€§"""
    num_nodes = graph.number_of_nodes()
    num_edges = graph.number_of_edges()
    edge_density = 2 * num_edges / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0

    validation_result = {
        "num_nodes": num_nodes,
        "num_edges": num_edges,
        "edge_density": edge_density,
        "is_feasible": num_nodes <= max_nodes_per_subgraph,
        "recommended_subgraphs": max(2, math.ceil(num_nodes / max_nodes_per_subgraph))
    }
    return validation_result

#æ‰§è¡Œç›¸å…³ç®—æ³•é€»è¾‘ï¼Œå¹¶å­˜å‚¨ç»“æœä¸ºcsv/logæ–‡ä»¶ï¼Œä¾¿äºåç»­åˆ†æ
def main_adapt(graphs, dataset, graph_index, seed):
    """Adapt-QAOA ä¸»å…¥å£ï¼šå®Œæ•´æµç¨‹ä¸è¾“å‡ºï¼Œå¸¦ç»Ÿä¸€æ ‡è¯†ä¾¿äºåŒºåˆ†"""
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    LOGS_DIR = os.path.join(BASE_DIR, "logs")
    CSV_DIR = os.path.join(BASE_DIR, "csvs")
    os.makedirs(LOGS_DIR, exist_ok=True)
    os.makedirs(CSV_DIR, exist_ok=True)
    # os.makedirs(os.path.join(BASE_DIR, "output"), exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "graph_visualizations"), exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "subgraph_visualizations"), exist_ok=True)

    subgraph_csv   = os.path.join(CSV_DIR, "adapt_subgraph_results.csv")
    graph_log_csv  = os.path.join(LOGS_DIR, "adapt_graph_results.log")
    if not os.path.exists(subgraph_csv):
        with open(subgraph_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "subgraph_index",
                "nodes", "edges", "min_k", "conflicts", "status", "processing_time"
            ])
    if not os.path.exists(graph_log_csv):
        with open(graph_log_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "nodes", "edges",
                "final_conflicts", "total_edges", "final_accuracy",
                "unique_colors", "global_max_k", "best_k_value",
                "subgraph_reoptimization_count", "processing_time",
                "conflict_changes", "total_time"
            ])

    all_results = []
    total_start = time.time()
    algo_params = {
        "n_qubits_per_node": 2,
        "learning_rate": 0.01,
        "max_k": 20,
        "p": 3,
        "num_steps": 1000,
        "max_iter": 10,
        "adjacency_threshold": 0.3,
        "early_stop_threshold": 3,
        "penalty": 1000,
        "Q": 20
    }

    for idx, graph in enumerate(graphs, start=1):
        np.random.seed(seed + idx)
        g_start = time.time()
        try:
            g_name   = getattr(graph, "file_name", f"graph_{idx}")
            base_ttl = os.path.splitext(g_name)[0]
            n_nodes  = graph.number_of_nodes()
            n_edges  = graph.number_of_edges()

            print(f"\n{'='*50}")
            # print(f"Adapt-QAOA | å›¾ {idx}/{len(graphs)}: {base_ttl}")
            print(f"adapt_qaoa | å›¾ {idx}/{len(graphs)}: {base_ttl}")
            print(f"èŠ‚ç‚¹: {n_nodes} | è¾¹: {n_edges}")
            print(f"{'='*50}")

            # 1 åŸå§‹å›¾
            try:
                plot_original_graph(
                    graph,
                    title=f"[Adapt-QAOA] {base_ttl} - Original Graph (Nodes: {n_nodes}, Edges: {n_edges})"
                )
            except Exception as e:
                handle_exception("plot_original_graph", idx, e)

            # 2 æ™ºèƒ½å­å›¾åˆ’åˆ†ï¼ˆé™åˆ¶é‡å­æ¯”ç‰¹æ•°æœ€å¤šä¸º21ï¼‰
            max_qubits = 21
            subgraphs, sub_maps, divide_info = smart_divide_graph_with_qubit_constraint(
                graph,
                max_qubits=max_qubits,
                max_k_per_subgraph=algo_params["max_k"],
                Q=algo_params["Q"]
            )
            print(f"Adapt-QAOA æ™ºèƒ½åˆ’åˆ†å®Œæˆï¼š{len(subgraphs)} ä¸ªå­å›¾ï¼ˆé‡å­æ¯”ç‰¹çº¦æŸï¼šâ‰¤{max_qubits}ï¼‰")

            # 3 å­å›¾å¯è§†åŒ–
            try:
                plot_New_IDs_subgraphs(
                    subgraphs, sub_maps,
                    title=f"[Adapt-QAOA] {base_ttl} - Subgraphs (Renumbered)"
                )
                plot_Original_IDs_subgraphs(
                    subgraphs,
                    title=f"[Adapt-QAOA] {base_ttl} - Subgraphs (Original IDs)"
                )
            except Exception as e:
                handle_exception("subgraph plotting", idx, e)

            # 4 å­å›¾ç€è‰²
            sub_start = time.time()
            sub_results = sequential_process_subgraphs(
                subgraphs=subgraphs,
                sub_mappings=sub_maps,
                dataset_name=dataset,
                graph_id=idx,
                max_k=algo_params["max_k"],
                p=algo_params["p"],
                num_steps=algo_params["num_steps"],
                vertex_colors=None,
                nodes_to_recolor=None,
                penalty=algo_params["penalty"],
                Q=algo_params["Q"],
                learning_rate=algo_params["learning_rate"]
            )
            sub_time = time.time() - sub_start
            min_k_list = [r[0] for r in sub_results if r and r[0] is not None]

            # å†™å­å›¾ CSV
            dataset_name = os.path.basename(getattr(graph, "file_name", "unknown").split(os.sep)[0])
            for s_idx, res in enumerate(sub_results):
                if res is None:
                    continue
                mk, _, conf, stat, _ = res
                sg = subgraphs[s_idx] if s_idx < len(subgraphs) else None
                with open(subgraph_csv, "a", newline="", encoding="utf-8") as f:
                    csv.writer(f).writerow([
                        dataset_name, g_name, idx, s_idx + 1,
                        sg.number_of_nodes() if sg else 0,
                        sg.number_of_edges() if sg else 0,
                        mk, conf, stat,
                        round(sub_time / len(subgraphs), 4) if subgraphs else 0
                    ])

            # 5 è¿­ä»£ä¼˜åŒ–
            opt_color, opt_acc, conf_counts, conf_hist, sub_opt_hist = iterative_optimization(
                graph=graph,
                subgraphs=subgraphs,
                sub_mappings=sub_maps,
                subgraph_results=sub_results,
                max_k=algo_params["max_k"],
                p=algo_params["p"],
                num_steps=algo_params["num_steps"],
                max_iter=algo_params["max_iter"],
                adjacency_threshold=algo_params["adjacency_threshold"],
                early_stop_threshold=algo_params["early_stop_threshold"],
                penalty=algo_params["penalty"],
                Q=algo_params["Q"],
                learning_rate=algo_params["learning_rate"],
                vertex_colors=None,
                nodes_to_recolor=None,
                dataset_name=dataset,
                graph_id=idx
            )

            final_color = opt_color
            uniq_colors = len(set(final_color.values())) if final_color else 0
            final_conf  = count_conflicts(final_color, graph) if final_color else -1
            reopt_cnt   = sum(1 for h in sub_opt_hist if isinstance(h, tuple) and len(h) >= 4 and h[3] > 0)
            best_k      = min(uniq_colors, max(min_k_list) if min_k_list else uniq_colors)

            print(f"\n===== Adapt-QAOA Optimization Summary =====")
            print(f"Final Conflicts: {final_conf} (Total Edges: {n_edges})")
            print(f"Final Accuracy: {opt_acc:.4f}")
            print(f"Colors Used: {uniq_colors}")
            print(f"Best k Value: {best_k}")
            print(f"Subgraph Reoptimization Count: {reopt_cnt}")

            # 6 å­å›¾ç€è‰²å¯è§†åŒ–
            try:
                sub_colorings = [
                    get_subgraph_coloring(sg, final_color, mk)
                    for sg, mk in zip(subgraphs, min_k_list)
                ]
                plot_New_IDs_colored_subgraphs(
                    subgraphs, sub_colorings, sub_maps, min_k_list,
                    title=f"[Adapt-QAOA] {base_ttl} - Subgraph Coloring (Renumbered)",
                    filename=f"adapt_qaoa_{base_ttl}",
                    output_dir=os.path.join(BASE_DIR, "subgraph_visualizations")
                )
                plot_Original_IDs_colored_subgraphs(
                    subgraphs, sub_colorings,
                    title=f"[Adapt-QAOA] {base_ttl} - Subgraph Coloring (Original IDs)",
                    min_k_list=min_k_list,
                    filename=f"adapt_qaoa_{base_ttl}",
                    output_dir=os.path.join(BASE_DIR, "subgraph_visualizations")
                )
            except Exception as e:
                print(f"Adapt-QAOA Subgraph coloring visualization failed: {e}")
                traceback.print_exc()

            # 7 è®¡ç®—å½“å‰å›¾çš„å¤„ç†æ—¶é—´
            g_time = time.time() - g_start

            # 8 æœ€ç»ˆå›¾å¯è§†åŒ–
            try:
                vis_title = (
                    f"[Adapt-QAOA] {base_ttl}\n"
                    f"Coloring Result (Colors: {uniq_colors}, "
                    f"Nodes: {n_nodes}, Edges: {n_edges}, Conflicts: {final_conf})"
                )
                visualize_graph(
                    graph, coloring=final_color, title=vis_title,
                    index=idx, min_k=uniq_colors, filename=f"adapt_{base_ttl}",
                    processing_time=g_time
                )
            except Exception as e:
                handle_exception("visualize_graph", idx, e)

            # 10. æ”¶é›†ç»“æœ
            all_results.append({
                "graph_index": idx,
                "graph": graph,
                "final_coloring": final_color,
                "subgraphs": subgraphs,
                "sub_mappings": sub_maps,
                "subgraph_results": sub_results,
                "sub_colorings": sub_colorings,
                "conflict_counts": conf_counts,
                "conflict_history": conf_hist,
                "subgraph_opt_history": sub_opt_hist,
                "unique_colors": uniq_colors,
                "final_conflicts": final_conf,
                "accuracy": opt_acc,
                "processing_time": g_time,
                "num_nodes": n_nodes,
                "num_edges": n_edges,
                "base_title": base_ttl,
                "global_max_k": algo_params["max_k"],
                "best_k_value": best_k,
                "reoptimization_count": reopt_cnt
            })

            # 10 å†™å…¨å±€æ—¥å¿—
            with open(graph_log_csv, "a", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow([
                    dataset_name, g_name, idx,
                    n_nodes, n_edges, final_conf, n_edges,
                    round(opt_acc, 4), uniq_colors,
                    algo_params["max_k"], best_k,
                    reopt_cnt, round(g_time, 4),
                    ",".join(map(str, conf_counts)) if conf_counts else "N/A",
                    round(time.time() - total_start, 4)
                ])
            print(f"Adapt-QAOA Graph {idx} completed, time: {g_time:.1f}s")

        except Exception as e:
            print(f"Adapt-QAOA å¤„ç†å›¾ {idx} å¼‚å¸¸: {e}")
            traceback.print_exc()
            continue

    # 11 ç”Ÿæˆ PDF æ±‡æ€»
    total_time = time.time() - total_start
    print(f"\n{'='*50}")
    print(f"Adapt-QAOA All completed, total time: {total_time:.1f}s")
    print(f"Successfully processed {len(all_results)}/{len(graphs)} graphs")
    print(f"Logs: {graph_log_csv} | CSV: {subgraph_csv}")
    print(f"{'='*50}")

    return all_results


# ----------- æµ‹è¯•ä¸»å‡½æ•° -----------
def parse_test_args():
    """Parse command line arguments for testing"""
    parser = argparse.ArgumentParser(
        description='QAOA Algorithm Testing Tool: Supports Adapt-QAOA, Standard-QAOA, and Noisy Adapt-QAOA'
    )
    parser.add_argument('--adapt', action='store_true', help='Test Adapt-QAOA algorithm')
    parser.add_argument('--standard', action='store_true', help='Test Standard-QAOA algorithm')
    parser.add_argument('--adapt-noise', action='store_true', help='Test Noisy Adapt-QAOA algorithm')
    parser.add_argument('--noise-prob', type=float, default=0.05,
                        help='Noise probability for noisy experiments (default: 0.05)')
    parser.add_argument('--seed', type=int, default=10, help='Random seed (default: 10)')
    parser.add_argument('--dataset', type=str, default='test_dataset', help='Dataset name')
    parser.add_argument('--graph-index', type=int, default=0, help='Graph index')
    parser.add_argument('--graph-dir', type=str, default=None,
                        help='Graph data directory path, load graph files from this directory if specified')
    parser.add_argument('--format-type', type=str, default='auto',
                        choices=['auto', 'col', 'pkl'],
                        help='Data loading format: auto(automatic), col(.col only), pkl(.pkl only) (default: auto)')
    parser.add_argument('--large-datasets', action='store_true',
                        help='Run on large-scale datasets (cora.col, citeseer.col, pubmed.col)')
    parser.add_argument('--run-classical', action='store_true',
                        help='Run classical algorithms (Greedy, Welch-Powell) for comparison')
    return parser.parse_args()


def main_test():
    """Main testing function: Execute selected QAOA algorithms"""
    args = parse_test_args()

    # Validate that at least one algorithm is selected
    if not any([args.adapt, args.standard, args.adapt_noise]):
        print("âš ï¸ Must select at least one algorithm (--adapt/--standard/--adapt-noise)")
        print("Usage examples:")
        print("  python Main_Multilevel_qaoa.py --adapt")
        print("  python Main_Multilevel_qaoa.py --standard")
        print("  python Main_Multilevel_qaoa.py --adapt-noise --noise-prob 0.1")
        print("  python Main_Multilevel_qaoa.py --adapt --standard --adapt-noise")
        return

    # Load test graphs
    print("\n" + "="*60)
    print("Loading test graph data...")
    print("="*60)

    if args.graph_dir:
        graph_dir = os.path.abspath(args.graph_dir)
        print(f"Loading graph data from directory: {graph_dir} (format: {args.format_type})")
        graphs = load_graphs_from_dir(graph_dir, format_type=args.format_type)
    else:
        # Use default directory (graph_loader will select based on format_type)
        print(f"Using default graph directory (format: {args.format_type})")
        graphs = load_graphs_from_dir('default', format_type=args.format_type)

    if not graphs:
        print("âš ï¸ No graph data loaded, exiting program")
        return {}

    print(f'ğŸ“¦ Total {len(graphs)} test graphs (seed {args.seed})')

    # Display test configuration
    print("\n" + "="*60)
    print("Test Configuration:")
    print("="*60)
    print(f"  - Adapt-QAOA: {'Enabled' if args.adapt else 'Disabled'}")
    print(f"  - Standard-QAOA: {'Enabled' if args.standard else 'Disabled'}")
    print(f"  - Noisy Adapt-QAOA: {'Enabled' if args.adapt_noise else 'Disabled'}")
    if args.adapt_noise:
        print(f"  - Noise Probability: {args.noise_prob}")
    print(f"  - Random Seed: {args.seed}")
    print("="*60)

    results = {}

    # Execute Adapt-QAOA
    if args.adapt:
        print("\n" + "="*60)
        print("Starting Adapt-QAOA test...")
        print("="*60)
        try:
            adapt_results = main_adapt(graphs, args.dataset, args.graph_index, args.seed)
            results['adapt'] = adapt_results
            print(f"\nâœ… Adapt-QAOA test completed, processed {len(adapt_results)} graphs")
        except Exception as e:
            print(f"\nâš ï¸ Adapt-QAOA test failed: {e}")
            import traceback
            traceback.print_exc()

    # Execute Standard-QAOA
    if args.standard:
        print("\n" + "="*60)
        print("Starting Standard-QAOA test...")
        print("="*60)
        try:
            standard_results = main_standard(graphs, args.dataset, args.graph_index, args.seed)
            results['standard'] = standard_results
            print(f"\nâœ… Standard-QAOA test completed, processed {len(standard_results)} graphs")
        except Exception as e:
            print(f"\nâš ï¸ Standard-QAOA test failed: {e}")
            import traceback
            traceback.print_exc()

    # Execute Noisy Adapt-QAOA
    if args.adapt_noise:
        print("\n" + "="*60)
        print("Starting Noisy Adapt-QAOA test...")
        print("="*60)
        try:
            noise_results = main_adapt_noise(
                graphs, args.dataset, args.graph_index, args.seed,
                depolarizing_prob=args.noise_prob
            )
            results['adapt_noise'] = noise_results
            print(f"\nâœ… Noisy Adapt-QAOA test completed, processed {len(noise_results)} graphs")
        except Exception as e:
            print(f"\nâš ï¸ Noisy Adapt-QAOA test failed: {e}")
            import traceback
            traceback.print_exc()

    # Output test results summary
    print("\n" + "="*60)
    print("Test Results Summary")
    print("="*60)
    for algo_name, algo_results in results.items():
        if algo_results:
            print(f"\n{algo_name.upper()}:")
            for r in algo_results:
                print(f"  Graph {r['graph_index']}: Colors={r['unique_colors']}, "
                      f"Conflicts={r['final_conflicts']}, "
                      f"Time={r['processing_time']:.2f}s, "
                      f"Accuracy={r['accuracy']:.4f}")
    print("\n" + "="*60)
    print("âœ… All tests completed")
    print("="*60)


    return results


# ---------- æ ‡å‡† QAOA ç‹¬ç«‹å…¥å£ ----------
def main_standard(graphs, dataset, graph_index, seed):
    """
    æ ‡å‡† QAOA å…¥å£ï¼Œæµç¨‹ä¸ adapt å®Œå…¨ä¸€è‡´ï¼Œä»…æŠŠå­å›¾å¤„ç†æ›¿æ¢æˆ standard ç³»åˆ—å‡½æ•°
    è¿”å›æ ¼å¼ä¸ main() å®Œå…¨ä¸€è‡´ï¼šlist[dict]
    """
    # ---- å¤ç”¨ adapt çš„ç›®å½•ã€æ—¥å¿—ã€å‚æ•°é…ç½® ----
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    LOGS_DIR = os.path.join(BASE_DIR, "logs")
    CSV_DIR = os.path.join(BASE_DIR, "csvs")
    os.makedirs(LOGS_DIR, exist_ok=True)
    os.makedirs(CSV_DIR, exist_ok=True)
    # os.makedirs(os.path.join(BASE_DIR, "output"), exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "graph_visualizations"), exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "subgraph_visualizations"), exist_ok=True)

    subgraph_csv = os.path.join(CSV_DIR, "standard_subgraph_results.csv")
    if not os.path.exists(subgraph_csv):
        with open(subgraph_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "subgraph_index",
                "nodes", "edges", "min_k", "conflicts", "status", "processing_time"
            ])

    graph_log_csv = os.path.join(LOGS_DIR, "standard_graph_results.log")
    if not os.path.exists(graph_log_csv):
        with open(graph_log_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "nodes", "edges",
                "final_conflicts", "total_edges", "final_accuracy",
                "unique_colors", "global_max_k", "best_k_value",
                "subgraph_reoptimization_count", "processing_time",
                "conflict_changes", "total_time"
            ])

    all_results = []
    total_start_time = time.time()
    algorithm_params = {
        "n_qubits_per_node": 2,
        "learning_rate": 0.01,
        "max_k": 20,
        "p": 1,
        "num_steps": 1000,
        "max_iter": 10,
        "adjacency_threshold": 0.3,
        "early_stop_threshold": 2,
        "penalty": 1000,
        "Q": 20
    }

    # ---------- å¼€å§‹é€å›¾å¤„ç† ----------
    for index, graph in enumerate(graphs, start=1):
        graph_start_time = time.time()
        try:
            graph_name = getattr(graph, "file_name", f"graph_{index}")
            base_title = os.path.splitext(graph_name)[0]
            num_nodes = graph.number_of_nodes()
            num_edges = graph.number_of_edges()

            print(f"\n{'='*50}")
            print(f"Processing Graph {index}/{len(graphs)}: {base_title} (Standard-QAOA)")
            print(f"Graph Properties: {num_nodes} Nodes, {num_edges} Edges")
            print(f"{'='*50}")

            # 1. Original graph visualization (reuse from adapt function)
            try:
                plot_original_graph(graph, title=f"{base_title} - Original Graph (Nodes: {num_nodes}, Edges: {num_edges})")
            except Exception as e:
                handle_exception("plot_original_graph", index, e)

            # 2. æ™ºèƒ½å­å›¾åˆ’åˆ†ï¼ˆé™åˆ¶é‡å­æ¯”ç‰¹æ•°æœ€å¤šä¸º21ï¼‰
            max_qubits = 21
            subgraphs, sub_mappings, divide_info = smart_divide_graph_with_qubit_constraint(
                graph,
                max_qubits=max_qubits,
                max_k_per_subgraph=algorithm_params["max_k"],
                Q=algorithm_params["Q"]
            )
            print(f"Standard-QAOA æ™ºèƒ½åˆ’åˆ†å®Œæˆï¼š{len(subgraphs)} ä¸ªå­å›¾ï¼ˆé‡å­æ¯”ç‰¹çº¦æŸï¼šâ‰¤{max_qubits}ï¼‰")

            # 3. Subgraph visualization (optional)
            try:
                plot_New_IDs_subgraphs(subgraphs, sub_mappings, title=f"{base_title} - Subgraphs (Renumbered)")
                plot_Original_IDs_subgraphs(subgraphs, title=f"{base_title} - Subgraphs (Original IDs)")
            except Exception as e:
                handle_exception("subgraph plotting", index, e)

            # 4. æ ‡å‡† QAOA å­å›¾å¤„ç†
            subgraph_start_time = time.time()
            subgraph_results = sequential_process_subgraphs_standard(
                subgraphs=subgraphs,
                sub_mappings=sub_mappings,
                dataset_name=dataset,
                graph_id=index,
                max_k=algorithm_params["max_k"],
                p=algorithm_params["p"],
                num_steps=algorithm_params["num_steps"],
                vertex_colors=None,
                nodes_to_recolor=None,
                penalty=algorithm_params["penalty"],
                Q=algorithm_params["Q"],
                learning_rate=algorithm_params["learning_rate"]
            )
            subgraph_total_time = time.time() - subgraph_start_time

            # è®°å½•å­å›¾çº§åˆ«ç»“æœåˆ° CSV
            dataset_name = os.path.basename(getattr(graph, "file_name", "unknown").split(os.sep)[0])
            for sub_idx, result in enumerate(subgraph_results):
                if result is None:
                    continue
                min_k, coloring, conflicts, status, _ = result
                subgraph = subgraphs[sub_idx] if sub_idx < len(subgraphs) else None
                sub_nodes = subgraph.number_of_nodes() if subgraph else 0
                sub_edges = subgraph.number_of_edges() if subgraph else 0
                with open(subgraph_csv, "a", newline="", encoding="utf-8") as f:
                    csv.writer(f).writerow([
                        dataset_name, graph_name, index, sub_idx + 1,
                        sub_nodes, sub_edges, min_k, conflicts, status,
                        round(subgraph_total_time / len(subgraphs), 4) if subgraphs else 0
                    ])

            # 5. æ ‡å‡† QAOA è¿­ä»£ä¼˜åŒ–
            optimized_coloring, opt_acc, conflict_counts, conflict_history, subgraph_opt_history = iterative_optimization_standard(
                graph=graph,
                subgraphs=subgraphs,
                sub_mappings=sub_mappings,
                subgraph_results=subgraph_results,
                max_k=algorithm_params["max_k"],
                p=algorithm_params["p"],
                num_steps=algorithm_params["num_steps"],
                max_iter=algorithm_params["max_iter"],
                adjacency_threshold=algorithm_params["adjacency_threshold"],
                early_stop_threshold=algorithm_params["early_stop_threshold"],
                penalty=algorithm_params["penalty"],
                Q=algorithm_params["Q"],
                learning_rate=algorithm_params["learning_rate"],
                vertex_colors=None,
                nodes_to_recolor=None,
                dataset_name=dataset,
                graph_id=index
            )

            # 6. ç»Ÿä¸€è¾“å‡ºé€»è¾‘ï¼ˆä¸ adapt å®Œå…¨ä¸€è‡´ï¼‰
            final_coloring = optimized_coloring
            unique_colors = len(set(final_coloring.values())) if final_coloring else 0
            final_conflicts = count_conflicts(final_coloring, graph) if final_coloring else -1
            reoptimization_count = sum(
                1 for h in subgraph_opt_history
                if isinstance(h, tuple) and len(h) >= 4 and h[3] > 0
            )
            min_k_list = [r[0] for r in subgraph_results if r is not None and r[0] is not None]
            best_k_value = min(unique_colors, max(min_k_list) if min_k_list else unique_colors)

            print(f"\n===== Optimization Summary (Standard-QAOA) =====")
            print(f"Final Conflicts: {final_conflicts} (Total Edges: {num_edges})")
            print(f"Final Accuracy: {opt_acc:.4f}")
            print(f"Colors Used: {unique_colors} (Global max_k limit: {algorithm_params['max_k']})")
            print(f"Best k Value: {best_k_value}")
            print(f"Subgraph Reoptimization Count: {reoptimization_count}")

            # 7. å­å›¾ç€è‰²å¯è§†åŒ–
            try:
                subgraph_colorings = [
                    get_subgraph_coloring(subgraph, final_coloring, mk)
                    for subgraph, mk in zip(subgraphs, min_k_list)
                ]
                plot_New_IDs_colored_subgraphs(
                    subgraphs, subgraph_colorings, sub_mappings, min_k_list,
                    title=f"{base_title} - å­å›¾ç€è‰²ï¼ˆæ–°ç¼–å·ï¼‰", filename=base_title,
                    output_dir=os.path.join(BASE_DIR, "subgraph_visualizations")
                )
                plot_Original_IDs_colored_subgraphs(
                    subgraphs, subgraph_colorings,
                    title=f"{base_title} - å­å›¾ç€è‰²ï¼ˆåŸå§‹ç¼–å·ï¼‰",
                    min_k_list=min_k_list, filename=base_title,
                    output_dir=os.path.join(BASE_DIR, "subgraph_visualizations")
                )
            except Exception as e:
                print(f"Error in colored subgraph plotting for graph {index}: {str(e)}")
                traceback.print_exc()

            # 8. è®¡ç®—å½“å‰å›¾çš„å¤„ç†æ—¶é—´
            graph_time = time.time() - graph_start_time

            # 9. Final graph visualization
            try:
                final_graph_title = (
                    f"{base_title}\n"
                    f"Coloring Result (Colors: {unique_colors}, "
                    f"Nodes: {num_nodes}, Edges: {num_edges}, "
                    f"Conflicts: {final_conflicts})"
                )
                visualize_graph(
                    graph, coloring=final_coloring, title=final_graph_title,
                    index=index, min_k=unique_colors, filename=base_title,
                    processing_time=graph_time
                )
            except Exception as e:
                handle_exception("visualize_graph", index, e)

            # 10. æ”¶é›†ç»“æœ
            result = {
                "graph_index": index,
                "graph": graph,
                "final_coloring": final_coloring,
                "subgraphs": subgraphs,
                "sub_mappings": sub_mappings,
                "subgraph_results": subgraph_results,
                "sub_colorings": subgraph_colorings,
                "conflict_counts": conflict_counts,
                "conflict_history": conflict_history,
                "subgraph_opt_history": subgraph_opt_history,
                "unique_colors": unique_colors,
                "final_conflicts": final_conflicts,
                "accuracy": opt_acc,
                "processing_time": graph_time,
                "num_nodes": num_nodes,
                "num_edges": num_edges,
                "base_title": base_title,
                "global_max_k": algorithm_params["max_k"],
                "best_k_value": best_k_value,
                "reoptimization_count": reoptimization_count
            }
            all_results.append(result)

            # 11. å†™å…¨å±€æ—¥å¿—
            with open(graph_log_csv, "a", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                conflict_changes_str = ",".join(map(str, conflict_counts)) if conflict_counts else "N/A"
                writer.writerow([
                    dataset_name, graph_name, index,
                    num_nodes, num_edges, final_conflicts, num_edges,
                    round(opt_acc, 4), unique_colors,
                    algorithm_params["max_k"], best_k_value,
                    reoptimization_count, round(graph_time, 4),
                    conflict_changes_str,
                    round(time.time() - total_start_time, 4)
                ])

        except Exception as e:
            print(f"Uncaught exception while processing graph {index}: {e}")
            traceback.print_exc()
            continue

    # ---------- Post-processing ----------
    # 12. è®¡ç®—æ€»æ—¶é—´
    total_time = time.time() - total_start_time
    print(f"\n{'='*50}")
    print(f"Standard-QAOA all graphs processed, total time: {total_time:.1f}s")
    print(f"Successfully processed {len(all_results)}/{len(graphs)} graphs")
    print(f"CSV: {subgraph_csv} | Logs: {graph_log_csv}")
    print(f"{'='*50}")
    return all_results


# ---------- å«å™ª QAOA ç‹¬ç«‹å…¥å£ ----------


def main_adapt_noise(graphs, dataset, graph_index, seed, depolarizing_prob=0.01):
    """
    å«å™ªè‡ªé€‚åº”QAOAä¸»å…¥å£ï¼šå®Œæ•´æµç¨‹ä¸è¾“å‡ºï¼Œæ”¯æŒé€€æåŒ–å™ªå£°æ¨¡æ‹Ÿ
    depolarizing_prob: é€€æåŒ–å™ªå£°æ¦‚ç‡ï¼ˆ0~1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼Œé»˜è®¤0.01ï¼‰
    """
    # éªŒè¯å™ªå£°æ¦‚ç‡æœ‰æ•ˆæ€§
    if depolarizing_prob is None or not (0 <= depolarizing_prob <= 1):
        raise ValueError(f"Invalid noise probability: {depolarizing_prob}, must be a float between 0 and 1")

    # Create output directory
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    LOGS_DIR = os.path.join(BASE_DIR, "logs")
    CSV_DIR = os.path.join(BASE_DIR, "csvs")
    os.makedirs(LOGS_DIR, exist_ok=True)
    os.makedirs(CSV_DIR, exist_ok=True)
    # os.makedirs(os.path.join(BASE_DIR, "output"), exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "graph_visualizations"), exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "subgraph_visualizations"), exist_ok=True)

    # Log file path (includes noise parameter identifier)
    noise_suffix = f"_noise_{depolarizing_prob:.3f}"
    subgraph_csv = os.path.join(CSV_DIR, f"adapt_noise_subgraph_results{noise_suffix}.csv")
    graph_log_csv = os.path.join(LOGS_DIR, f"adapt_noise_graph_results{noise_suffix}.log")

    # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
    if not os.path.exists(subgraph_csv):
        with open(subgraph_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "subgraph_index",
                "nodes", "edges", "min_k", "conflicts", "status",
                "processing_time", "depolarizing_prob"
            ])
    if not os.path.exists(graph_log_csv):
        with open(graph_log_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "nodes", "edges",
                "final_conflicts", "total_edges", "final_accuracy",
                "unique_colors", "global_max_k", "best_k_value",
                "subgraph_reoptimization_count", "processing_time",
                "conflict_changes", "total_time", "depolarizing_prob"
            ])

    all_results = []
    total_start = time.time()

    # Algorithm parameter configuration (includes noise parameter)
    algo_params = {
        "n_qubits_per_node": 2,
        "learning_rate": 0.01,
        "max_k": 20,
        "p": 3,  # QAOA layers
        "num_steps": 1000,
        "max_iter": 10,
        "adjacency_threshold": 0.3,
        "early_stop_threshold": 5,  # Relaxed early stop condition for noise scenario
        "penalty": 1000,
        "Q": 20,
        "depolarizing_prob": depolarizing_prob  # Depolarizing noise probability (ensure not None)
    }

    # Process each graph
    for idx, graph in enumerate(graphs, start=1):
        np.random.seed(seed + idx)  # Fix random seed for reproducibility
        g_start = time.time()
        try:
            # Get basic graph information
            g_name = getattr(graph, "file_name", f"graph_{idx}")
            base_ttl = os.path.splitext(g_name)[0]
            n_nodes = graph.number_of_nodes()
            n_edges = graph.number_of_edges()

            print(f"\n{'='*50}")
            print(f"Noisy Adapt-QAOA | Graph {idx}/{len(graphs)}: {base_ttl}")
            print(f"Nodes: {n_nodes} | Edges: {n_edges} | Noise Probability: {depolarizing_prob}")
            print(f"{'='*50}")

            # 1. Draw original graph
            try:
                plot_original_graph(
                    graph,
                    title=f"[Noisy Adapt-QAOA] {base_ttl} - Original Graph "
                          f"(Nodes: {n_nodes}, Edges: {n_edges}, Noise: {depolarizing_prob})"
                )
            except Exception as e:
                handle_exception("plot_original_graph", idx, e)

            # 2. æ™ºèƒ½å­å›¾åˆ’åˆ†ï¼ˆé™åˆ¶é‡å­æ¯”ç‰¹æ•°æœ€å¤šä¸º21ï¼‰
            max_qubits = 21
            subgraphs, sub_maps, divide_info = smart_divide_graph_with_qubit_constraint(
                graph,
                max_qubits=max_qubits,
                max_k_per_subgraph=algo_params["max_k"],
                Q=algo_params["Q"]
            )
            print(f"Noisy Adapt-QAOA æ™ºèƒ½åˆ’åˆ†å®Œæˆï¼š{len(subgraphs)} ä¸ªå­å›¾ï¼ˆé‡å­æ¯”ç‰¹çº¦æŸï¼šâ‰¤{max_qubits}ï¼‰")

            # 3. Subgraph visualization
            try:
                plot_New_IDs_subgraphs(
                    subgraphs, sub_maps,
                    title=f"[Adapt-noise-QAOA] {base_ttl} - Subgraphs (Renumbered)"
                )
                plot_Original_IDs_subgraphs(
                    subgraphs,
                    title=f"[Adapt-noise-QAOA] {base_ttl} - Subgraphs (Original IDs)"
                )
            except Exception as e:
                handle_exception("subgraph plotting", idx, e)

            # 4. å«å™ªå­å›¾ç€è‰²å¤„ç†
            sub_start = time.time()
            sub_results = sequential_process_subgraphs_noise(
                subgraphs=subgraphs,
                sub_mappings=sub_maps,
                dataset_name=dataset,
                graph_id=idx,
                max_k=algo_params["max_k"],
                p=algo_params["p"],
                num_steps=algo_params["num_steps"],
                vertex_colors=None,
                nodes_to_recolor=None,
                penalty=algo_params["penalty"],
                Q=algo_params["Q"],
                learning_rate=algo_params["learning_rate"],
                depolarizing_prob=algo_params["depolarizing_prob"]  # ä¼ é€’å™ªå£°å‚æ•°
            )
            sub_time = time.time() - sub_start
            min_k_list = [r[0] for r in sub_results if r and r[0] is not None]

            # å†™å…¥å­å›¾æ—¥å¿—
            dataset_name = os.path.basename(getattr(graph, "file_name", "unknown").split(os.sep)[0])
            for s_idx, res in enumerate(sub_results):
                if res is None:
                    continue
                mk, _, conf, stat, _ = res
                sg = subgraphs[s_idx] if s_idx < len(subgraphs) else None
                with open(subgraph_csv, "a", newline="", encoding="utf-8") as f:
                    csv.writer(f).writerow([
                        dataset_name, g_name, idx, s_idx + 1,
                        sg.number_of_nodes() if sg else 0,
                        sg.number_of_edges() if sg else 0,
                        mk, conf, stat,
                        round(sub_time / len(subgraphs), 4) if subgraphs else 0,
                        depolarizing_prob  # è®°å½•å™ªå£°æ¦‚ç‡
                    ])

            # 5. å«å™ªè¿­ä»£ä¼˜åŒ–
            opt_color, opt_acc, conf_counts, conf_hist, sub_opt_hist = iterative_optimization_noise(
                graph=graph,
                subgraphs=subgraphs,
                sub_mappings=sub_maps,
                subgraph_results=sub_results,
                max_k=algo_params["max_k"],
                p=algo_params["p"],
                num_steps=algo_params["num_steps"],
                max_iter=algo_params["max_iter"],
                adjacency_threshold=algo_params["adjacency_threshold"],
                early_stop_threshold=algo_params["early_stop_threshold"],
                penalty=algo_params["penalty"],
                Q=algo_params["Q"],
                learning_rate=algo_params["learning_rate"],
                vertex_colors=None,
                nodes_to_recolor=None,
                dataset_name=dataset,
                graph_id=idx,
                depolarizing_prob=algo_params["depolarizing_prob"]  # ä¼ é€’å™ªå£°å‚æ•°
            )

            # 6. ç»“æœç»Ÿè®¡
            final_color = opt_color
            uniq_colors = len(set(final_color.values())) if final_color else 0
            final_conf = count_conflicts(final_color, graph) if final_color else -1
            reopt_cnt = sum(1 for h in sub_opt_hist if isinstance(h, tuple) and len(h) >= 4 and h[3] > 0)
            best_k = min(uniq_colors, max(min_k_list) if min_k_list else uniq_colors)

            print(f"\n===== Adapt-noise-QAOA Optimization Summary =====")
            print(f"Final Conflicts: {final_conf} (Total Edges: {n_edges})")
            print(f"Final Accuracy: {opt_acc:.4f}")
            print(f"Colors Used: {uniq_colors}")
            print(f"Best k Value: {best_k}")
            print(f"Subgraph Reoptimization Count: {reopt_cnt}")
            print(f"Noise Parameter: Depolarizing Probability = {depolarizing_prob}")

            # 7. å­å›¾ç€è‰²å¯è§†åŒ–
            try:
                sub_colorings = [
                    get_subgraph_coloring(sg, final_color, mk)
                    for sg, mk in zip(subgraphs, min_k_list)
                ]
                plot_New_IDs_colored_subgraphs(
                    subgraphs, sub_colorings, sub_maps, min_k_list,
                    title=f"[Adapt-noise-QAOA] {base_ttl} - å­å›¾ç€è‰²ï¼ˆæ–°ç¼–å·ï¼‰",
                    filename=f"Adapt_noise_{base_ttl}_p{depolarizing_prob:.3f}",
                    output_dir=os.path.join(BASE_DIR, "subgraph_visualizations")
                )
                plot_Original_IDs_colored_subgraphs(
                    subgraphs, sub_colorings,
                    title=f"[Adapt-noise-QAOA] {base_ttl} - å­å›¾ç€è‰²ï¼ˆåŸå§‹ç¼–å·ï¼‰",
                    min_k_list=min_k_list,
                    filename=f"Adapt_noise_{base_ttl}_p{depolarizing_prob:.3f}",
                    output_dir=os.path.join(BASE_DIR, "subgraph_visualizations")
                )
            except Exception as e:
                print(f"Adapt-noise-QAOA subgraph coloring visualization failed: {e}")
                traceback.print_exc()

            # 8. è®¡ç®—å½“å‰å›¾çš„å¤„ç†æ—¶é—´
            g_time = time.time() - g_start

            # 9. Final graph visualization
            try:
                vis_title = (
                    f"[Adapt-noise-QAOA] {base_ttl}\n"
                    f"coloring result(colors: {uniq_colors}, nodes: {n_nodes}, edges: {n_edges}, "
                    f"conflicts: {final_conf}, probability noise: {depolarizing_prob})"
                )
                visualize_graph(
                    graph, coloring=final_color, title=vis_title,
                    index=idx, min_k=uniq_colors,
                    filename=f"Adapt_noise_{base_ttl}_p{depolarizing_prob:.3f}",
                    processing_time=g_time
                )
            except Exception as e:
                handle_exception("visualize_graph", idx, e)

            # 10. æ”¶é›†ç»“æœ
            all_results.append({
                "graph_index": idx,
                "graph": graph,
                "final_coloring": final_color,
                "subgraphs": subgraphs,
                "sub_mappings": sub_maps,
                "subgraph_results": sub_results,
                "sub_colorings": sub_colorings,
                "conflict_counts": conf_counts,
                "conflict_history": conf_hist,
                "subgraph_opt_history": sub_opt_hist,
                "unique_colors": uniq_colors,
                "final_conflicts": final_conf,
                "accuracy": opt_acc,
                "processing_time": g_time,
                "num_nodes": n_nodes,
                "num_edges": n_edges,
                "base_title": base_ttl,
                "global_max_k": algo_params["max_k"],
                "best_k_value": best_k,
                "reoptimization_count": reopt_cnt,
                "noise_params": {"depolarizing_prob": depolarizing_prob}
            })

            # 11. å†™å…¥å…¨å±€æ—¥å¿—
            with open(graph_log_csv, "a", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow([
                    dataset_name, g_name, idx,
                    n_nodes, n_edges, final_conf, n_edges,
                    round(opt_acc, 4), uniq_colors,
                    algo_params["max_k"], best_k,
                    reopt_cnt, round(g_time, 4),
                    ",".join(map(str, conf_counts)) if conf_counts else "N/A",
                    round(time.time() - total_start, 4),
                    depolarizing_prob  # è®°å½•å™ªå£°æ¦‚ç‡
                ])
            print(f"Adapt-noise-QAOA {idx} å®Œæˆï¼Œè€—æ—¶: {g_time:.1f}ç§’")

        except Exception as e:
            print(f"Adapt-noise-QAOA processing graph {idx} exception: {e}")
            traceback.print_exc()
            continue

    # 12. å®éªŒæ±‡æ€»
    total_time = time.time() - total_start
    print(f"\n{'='*50}")
    print(f"Adapt-noise-QAOA all completed, total time: {total_time:.1f}s")
    print(f"Noise Parameter: Depolarizing Probability = {depolarizing_prob}")
    print(f"Successfully processed {len(all_results)}/{len(graphs)} graphs")
    print(f"CSV: {subgraph_csv} | Logs: {graph_log_csv}")
    print(f"{'='*50}")

    return all_results


# # ============ ä½¿ç”¨ç¤ºä¾‹ ============
# # Main_Multilevel_qaoa.py ä½¿ç”¨ç¤ºä¾‹
# python Main_Multilevel_qaoa.py --adapt --format-type col
# python Main_Multilevel_qaoa.py --standard --format-type pkl


# ============================================================================
# ä¸»å‡½æ•°å…¥å£ï¼ˆæŒ‡å®šå…·ä½“æ•°æ®æ‰§è¡Œç€è‰²å¯åˆ‡æ¢è¯¦ç»†çš„run_experimentsï¼‰
# ============================================================================

def main():
    """
    ä¸»å‡½æ•°ï¼šæä¾›äº¤äº’å¼èœå•é€‰æ‹© QAOA ç®—æ³•

    æ”¯æŒçš„ç®—æ³•:
    1. Adapt-QAOA: è‡ªé€‚åº” QAOA
    2. Standard-QAOA: æ ‡å‡† QAOA
    3. Adapt-QAOA with Noise: å«å™ªè‡ªé€‚åº” QAOA

    æ•°æ®æ ¼å¼:
    - col: .col æ ¼å¼å›¾æ–‡ä»¶ï¼ˆä½äº Data/instances/ï¼‰
    - pkl: .pkl æ ¼å¼å›¾æ–‡ä»¶ï¼ˆä½äº Data/instances/temp2/ï¼‰
    - auto: è‡ªåŠ¨é€‰æ‹©ï¼ˆä¼˜å…ˆ .colï¼Œå¦åˆ™ .pklï¼‰
    """
    import sys

    print("\n" + "="*70)
    print("        QAOA Graph Coloring Algorithm Testing Tool")
    print("="*70)

    # Display menu
    print("\nPlease select the algorithm to run:")
    print("  1. Adapt-QAOA (Adaptive QAOA)")
    print("  2. Standard-QAOA (Standard QAOA)")
    print("  3. Adapt-QAOA with Noise (Noisy Adaptive QAOA)")
    print("  4. Run All Algorithms (Adapt + Standard + Noisy)")
    print("  0. Exit")

    try:
        choice = input("\nPlease enter option (0-4): ").strip()
    except (EOFError, KeyboardInterrupt):
        print("\n\nProgram exited")
        sys.exit(0)

    if choice == '0':
        print("Program exited")
        sys.exit(0)

    # Select data format
    print("\nPlease select data format:")
    print("  1. auto (Automatic selection: prioritize .col, otherwise .pkl)")
    print("  2. col (.col files only)")
    print("  3. pkl (.pkl files only)")
    try:
        format_choice = input("Please enter option (1-3, default=1): ").strip() or '1'
    except (EOFError, KeyboardInterrupt):
        print("\n\nProgram exited")
        sys.exit(0)

    format_map = {'1': 'auto', '2': 'col', '3': 'pkl'}
    format_type = format_map.get(format_choice, 'auto')

    # Select noise probability (only needed for noisy algorithm)
    noise_prob = 0.05
    if choice in ['3', '4']:
        try:
            noise_input = input(f"\nPlease enter noise probability (0-1, default=0.05): ").strip()
            if noise_input:
                noise_prob = float(noise_input)
                if not 0 <= noise_prob <= 1:
                    print("âš ï¸ Noise probability out of range, using default value 0.05")
                    noise_prob = 0.05
        except (EOFError, KeyboardInterrupt, ValueError):
            noise_prob = 0.05

    # Set experiment parameters
    seed = 10

    # Load graph data
    print("\n" + "="*70)
    print("Loading graph data...")
    print("="*70)
    graphs = load_graphs_from_dir('default', format_type=format_type)

    if not graphs:
        print("âš ï¸ No graph data loaded, program exiting")
        print(f"   Hint: Please ensure there are corresponding files in Data/instances/ or Data/instances/temp2/ directories")
        sys.exit(1)

    print(f'âœ“ Successfully loaded {len(graphs)} graphs (random seed: {seed})')

    # Store all algorithm results
    all_results = {}

    # æ‰§è¡Œé€‰ä¸­çš„ç®—æ³•
    if choice == '1' or choice == '4':
        # è¿è¡Œ Adapt-QAOA
        print("\n" + "="*70)
        print("Running Adapt-QAOA...")
        print("="*70)
        try:
            adapt_results = main_adapt(graphs, dataset, 0, seed)
            all_results['Adapt-QAOA'] = adapt_results
            print(f"\nâœ… Adapt-QAOA completed, successfully processed {len(adapt_results)} graphs")
        except Exception as e:
            print(f"\nâš ï¸ Adapt-QAOA execution failed: {e}")
            import traceback
            traceback.print_exc()

    if choice == '2' or choice == '4':
        # Run Standard-QAOA
        print("\n" + "="*70)
        print("Running Standard-QAOA...")
        print("="*70)
        try:
            standard_results = main_standard(graphs, dataset, 0, seed)
            all_results['Standard-QAOA'] = standard_results
            print(f"\nâœ… Standard-QAOA completed, successfully processed {len(standard_results)} graphs")
        except Exception as e:
            print(f"\nâš ï¸ Standard-QAOA execution failed: {e}")
            import traceback
            traceback.print_exc()

    if choice == '3' or choice == '4':
        # Run Noisy Adapt-QAOA
        print("\n" + "="*70)
        print(f"Running Adapt-QAOA with Noise (Noise Probability: {noise_prob})...")
        print("="*70)
        try:
            noise_results = main_adapt_noise(graphs, dataset, 0, seed, depolarizing_prob=noise_prob)
            all_results['Adapt-QAOA-Noise'] = noise_results
            print(f"\nâœ… Noisy Adapt-QAOA completed, successfully processed {len(noise_results)} graphs")
        except Exception as e:
            print(f"\nâš ï¸ Noisy Adapt-QAOA execution failed: {e}")
            import traceback
            traceback.print_exc()

    # Output results summary
    print("\n" + "="*70)
    print("                    Experiment Results Summary")
    print("="*70)

    for algo_name, results in all_results.items():
        if not results:
            continue

        print(f"\nã€{algo_name}ã€‘")
        print("-" * 70)
        print(f"  Successfully Processed Graphs: {len(results)}")
        print(f"  {'Graph Index':<12} {'Colors':<10} {'Conflicts':<10} {'Accuracy':<12} {'Time(s)':<10}")
        print("  " + "-" * 60)

        for r in results:
            idx = r['graph_index']
            colors = r['unique_colors']
            conflicts = r['final_conflicts']
            accuracy = r['accuracy']
            time_cost = r['processing_time']
            print(f"  {idx:<12} {colors:<10} {conflicts:<10} {accuracy:<12.4f} {time_cost:<10.2f}")

        # Calculate statistics
        avg_colors = sum(r['unique_colors'] for r in results) / len(results)
        avg_time = sum(r['processing_time'] for r in results) / len(results)
        avg_accuracy = sum(r['accuracy'] for r in results) / len(results)
        total_conflicts = sum(r['final_conflicts'] for r in results)

        print("  " + "-" * 60)
        print(f"  Average Colors: {avg_colors:.2f}")
        print(f"  Average Time: {avg_time:.2f} s")
        print(f"  Average Accuracy: {avg_accuracy:.4f}")
        print(f"  Total Conflicts: {total_conflicts}")

    # Comparison summary (if multiple algorithms were run)
    if len(all_results) > 1:
        print("\n" + "="*70)
        print("                    Algorithm Comparison")
        print("="*70)
        print(f"  {'Algorithm Name':<20} {'Avg Colors':<15} {'Avg Accuracy':<15} {'Avg Time(s)':<15}")
        print("  " + "-" * 65)

        for algo_name, results in all_results.items():
            if results:
                avg_colors = sum(r['unique_colors'] for r in results) / len(results)
                avg_accuracy = sum(r['accuracy'] for r in results) / len(results)
                avg_time = sum(r['processing_time'] for r in results) / len(results)
                print(f"  {algo_name:<20} {avg_colors:<15.2f} {avg_accuracy:<15.4f} {avg_time:<15.2f}")

    print("\n" + "="*70)
    print("Experiment completed! Log files have been saved to logs/ directory")
    print("="*70 + "\n")


# ========================================================================
# å¤§è§„æ¨¡æ•°æ®é›†å¤„ç†å‡½æ•°
# ========================================================================

def load_large_datasets():
    """åŠ è½½å¤§è§„æ¨¡æ•°æ®é›†"""
    # æ•°æ®ç›®å½•: HAdaQAOA/Data/Large_datesets/
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    DATA_DIR = os.path.join(BASE_DIR, "Data", "Large_datesets")

    large_datasets = [
        # "cora.col",
        "citeseer.col",
        "pubmed.col"
    ]

    print(f"\nåŠ è½½å¤§è§„æ¨¡å›¾æ•°æ®...")
    print(f"  æ•°æ®ç›®å½•: {DATA_DIR}")

    graphs = []
    for dataset_name in large_datasets:
        dataset_path = os.path.join(DATA_DIR, dataset_name)
        if os.path.exists(dataset_path):
            print(f"  æ­£åœ¨åŠ è½½: {dataset_name}")
            try:
                graph = read_col_file(dataset_path)
                if graph is not None:
                    graph.file_name = dataset_name
                    graphs.append(graph)
                    print(f"    âœ“ æˆåŠŸåŠ è½½ {dataset_name}: {graph.number_of_nodes()} èŠ‚ç‚¹, {graph.number_of_edges()} è¾¹")
                else:
                    print(f"    âœ— åŠ è½½å¤±è´¥: {dataset_name}")
            except Exception as e:
                print(f"    âœ— åŠ è½½ {dataset_name} æ—¶å‡ºé”™: {e}")
                traceback.print_exc()
        else:
            print(f"  âœ— æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")

    return graphs


def print_large_dataset_analysis(graphs, max_nodes_per_subgraph):
    """æ‰“å°å¤§è§„æ¨¡æ•°æ®é›†åˆ†æ"""
    print(f"\nğŸ“Š å¤§è§„æ¨¡å›¾æ•°æ®åˆ†æ:")
    print(f"{'å›¾å':<15} {'èŠ‚ç‚¹':<8} {'è¾¹':<10} {'è¾¹å¯†åº¦':<12} {'å¯è¡Œæ€§':<8} {'æ¨èå­å›¾æ•°'}")
    print("â”€" * 75)

    for i, g in enumerate(graphs, 1):
        g_name = getattr(g, 'file_name', f'graph_{i}')
        base_name = os.path.splitext(g_name)[0]

        validation = validate_graph_feasibility(g, max_nodes_per_subgraph)
        feasible_str = "âœ“ å¯è¡Œ" if validation['is_feasible'] else "âœ— éœ€åˆ’åˆ†"

        print(f"{base_name:<15} {validation['num_nodes']:<8} {validation['num_edges']:<10} "
              f"{validation['edge_density']:<12.6f} {feasible_str:<8} "
              f"{validation['recommended_subgraphs']}")
    print("â”€" * 75)


def main_adapt_large(graphs, dataset, graph_index, seed, run_classical=False):
    """
    Adapt-QAOA å¤§è§„æ¨¡æ•°æ®é›†å…¥å£ï¼ˆåŠ è½½å¤§è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨æ™ºèƒ½åˆ’åˆ†ï¼‰
    """
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    LOGS_DIR = os.path.join(BASE_DIR, "logs")
    CSV_DIR = os.path.join(BASE_DIR, "csvs")
    os.makedirs(LOGS_DIR, exist_ok=True)
    os.makedirs(CSV_DIR, exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "large_graph_visualizations"), exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "large_subgraph_visualizations"), exist_ok=True)

    subgraph_csv = os.path.join(CSV_DIR, "large_datasets_adapt_subgraph_results.csv")
    if not os.path.exists(subgraph_csv):
        with open(subgraph_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "subgraph_index",
                "nodes", "edges", "min_k", "conflicts", "status", "processing_time"
            ])

    graph_log_csv = os.path.join(LOGS_DIR, "large_datasets_adapt_graph_results.log")
    if not os.path.exists(graph_log_csv):
        with open(graph_log_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "nodes", "edges",
                "final_conflicts", "total_edges", "final_accuracy",
                "unique_colors", "global_max_k", "best_k_value",
                "subgraph_reoptimization_count", "processing_time",
                "conflict_changes", "total_time"
            ])

    # åˆ›å»ºç»å…¸ç®—æ³•ç»“æœCSV
    classical_csv = os.path.join(CSV_DIR, "large_datasets_adapt_classical_results.csv")
    if run_classical and not os.path.exists(classical_csv):
        with open(classical_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "algorithm",
                "num_nodes", "num_edges", "num_colors", "conflicts",
                "is_valid", "execution_time_ms"
            ])

    # é…ç½®æ—¥å¿—
    log_file = os.path.join(LOGS_DIR, "large_adapt_qaoa.log")
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logger = logging.getLogger(__name__)

    all_results = []
    total_start_time = time.time()

    # ç®—æ³•å‚æ•°é…ç½® - é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®é›†ä¼˜åŒ–
    algorithm_params = {
        "max_k": 8,
        "p": 3,
        "num_steps": 500,
        "max_iter": 15,
        "early_stop_threshold": 5,
        "penalty": 1000,
        "Q": 20
    }

    for index, graph in enumerate(graphs, start=1):
        graph_start_time = time.time()
        try:
            graph_name = getattr(graph, "file_name", f"graph_{index}")
            base_title = os.path.splitext(graph_name)[0]
            num_nodes = graph.number_of_nodes()
            num_edges = graph.number_of_edges()

            print(f"\n{'='*60}")
            print(f"Processing Graph {index}/{len(graphs)}: {base_title} (Adapt-QAOA Large)")
            print(f"Graph Properties: {num_nodes} Nodes, {num_edges} Edges")
            print(f"{'='*60}")

            # 0. è¿è¡Œç»å…¸ç®—æ³•å¯¹æ¯”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            classical_results = {}
            if run_classical:
                print(f"\n{'â”€'*50}")
                print(f"Running Classical Algorithms Comparison")
                print(f"{'â”€'*50}")

                greedy_res = run_greedy_coloring(graph, graph_name)
                if greedy_res:
                    print(f"âœ“ Greedy Algorithm:")
                    print(f"  Colors: {greedy_res['num_colors']}, Conflicts: {greedy_res['conflicts']}, "
                          f"Time: {greedy_res['execution_time_ms']:.2f}ms")
                    classical_results['greedy'] = greedy_res

                    dataset_name = os.path.basename(getattr(graph, "file_name", "unknown").split(os.sep)[0])
                    with open(classical_csv, "a", newline="", encoding="utf-8") as f:
                        csv.writer(f).writerow([
                            dataset_name, graph_name, index, greedy_res['algorithm'],
                            greedy_res['num_nodes'], greedy_res['num_edges'],
                            greedy_res['num_colors'], greedy_res['conflicts'],
                            greedy_res['is_valid'], greedy_res['execution_time_ms']
                        ])

                wp_res = run_welch_powell_coloring(graph, graph_name)
                if wp_res:
                    print(f"âœ“ Welch-Powell Algorithm:")
                    print(f"  Colors: {wp_res['num_colors']}, Conflicts: {wp_res['conflicts']}, "
                          f"Time: {wp_res['execution_time_ms']:.2f}ms")
                    classical_results['welch_powell'] = wp_res

                    with open(classical_csv, "a", newline="", encoding="utf-8") as f:
                        csv.writer(f).writerow([
                            dataset_name, graph_name, index, wp_res['algorithm'],
                            wp_res['num_nodes'], wp_res['num_edges'],
                            wp_res['num_colors'], wp_res['conflicts'],
                            wp_res['is_valid'], wp_res['execution_time_ms']
                        ])

            # 1. åŸå§‹å›¾å¯è§†åŒ–
            try:
                filename = f"{base_title}_original"
                plot_original_graph(
                    graph,
                    title=f"[Adapt-QAOA] {base_title} - Original Graph (Nodes: {num_nodes}, Edges: {num_edges})",
                    filename=filename,
                    output_dir=os.path.join(BASE_DIR, "large_graph_visualizations")
                )
            except Exception as e:
                handle_exception("plot_original_graph", index, e)

            # 2. æ™ºèƒ½å­å›¾åˆ’åˆ†ï¼ˆé™åˆ¶é‡å­æ¯”ç‰¹æ•°æœ€å¤šä¸º21ï¼‰
            max_qubits = 21
            subgraphs, sub_mappings, divide_info = smart_divide_graph_with_qubit_constraint(
                graph,
                max_qubits=max_qubits,
                max_k_per_subgraph=algorithm_params["max_k"],
                Q=algorithm_params["Q"]
            )
            logger.info(f"Adapt-QAOA æ™ºèƒ½åˆ’åˆ†å®Œæˆ: {len(subgraphs)} ä¸ªå­å›¾ï¼ˆé‡å­æ¯”ç‰¹çº¦æŸï¼šâ‰¤{max_qubits}ï¼‰")

            # 3. å­å›¾å¯è§†åŒ–
            try:
                plot_New_IDs_subgraphs(
                    subgraphs, sub_mappings,
                    title=f"[Adapt-QAOA] {base_title} - Subgraphs (Renumbered)",
                    filename=f"{base_title}_subgraphs_renumbered",
                    output_dir=os.path.join(BASE_DIR, "large_subgraph_visualizations")
                )
                plot_Original_IDs_subgraphs(
                    subgraphs,
                    title=f"[Adapt-QAOA] {base_title} - Subgraphs (Original IDs)",
                    filename=f"{base_title}_subgraphs_original",
                    output_dir=os.path.join(BASE_DIR, "large_subgraph_visualizations")
                )
            except Exception as e:
                handle_exception("subgraph plotting", index, e)

            # 4. Adapt-QAOA å­å›¾å¤„ç†
            subgraph_start_time = time.time()
            subgraph_results = sequential_process_subgraphs(
                subgraphs=subgraphs,
                sub_mappings=sub_mappings,
                dataset_name=dataset,
                graph_id=index,
                max_k=algorithm_params["max_k"],
                p=algorithm_params["p"],
                num_steps=algorithm_params["num_steps"],
                vertex_colors=None,
                nodes_to_recolor=None,
                penalty=algorithm_params["penalty"],
                Q=algorithm_params["Q"],
                learning_rate=0.01
            )
            subgraph_total_time = time.time() - subgraph_start_time

            # è®°å½•å­å›¾çº§åˆ«ç»“æœ
            dataset_name = os.path.basename(getattr(graph, "file_name", "unknown").split(os.sep)[0])
            for sub_idx, result in enumerate(subgraph_results):
                if result is None:
                    continue
                min_k, coloring, conflicts, status, _ = result
                subgraph = subgraphs[sub_idx] if sub_idx < len(subgraphs) else None
                sub_nodes = subgraph.number_of_nodes() if subgraph else 0
                sub_edges = subgraph.number_of_edges() if subgraph else 0
                with open(subgraph_csv, "a", newline="", encoding="utf-8") as f:
                    csv.writer(f).writerow([
                        dataset_name, graph_name, index, sub_idx + 1,
                        sub_nodes, sub_edges, min_k, conflicts, status,
                        round(subgraph_total_time / len(subgraphs), 4) if subgraphs else 0
                    ])

            # 5. Adapt-QAOA è¿­ä»£ä¼˜åŒ–
            opt_color, opt_acc, conflict_counts, conflict_history, sub_opt_hist = iterative_optimization(
                graph=graph,
                subgraphs=subgraphs,
                sub_mappings=sub_mappings,
                subgraph_results=subgraph_results,
                max_k=algorithm_params["max_k"],
                p=algorithm_params["p"],
                num_steps=algorithm_params["num_steps"],
                max_iter=algorithm_params["max_iter"],
                adjacency_threshold=0.3,
                early_stop_threshold=algorithm_params["early_stop_threshold"],
                penalty=algorithm_params["penalty"],
                Q=algorithm_params["Q"],
                learning_rate=0.01,
                vertex_colors=None,
                nodes_to_recolor=None,
                dataset_name=dataset,
                graph_id=index
            )

            # 6. ç»Ÿè®¡ç»“æœ
            final_coloring = opt_color
            unique_colors = len(set(final_coloring.values())) if final_coloring else 0
            final_conflicts = count_conflicts(final_coloring, graph) if final_coloring else -1
            reoptimization_count = sum(
                1 for h in sub_opt_hist
                if isinstance(h, tuple) and len(h) >= 4 and h[3] > 0
            )
            min_k_list = [r[0] for r in subgraph_results if r is not None and r[0] is not None]
            best_k_value = min(unique_colors, max(min_k_list) if min_k_list else unique_colors)

            print(f"\n===== Optimization Summary (Adapt-QAOA Large) =====")
            print(f"Final Conflicts: {final_conflicts} (Total Edges: {num_edges})")
            print(f"Final Accuracy: {opt_acc:.4f}")
            print(f"Colors Used: {unique_colors} (Global max_k limit: {algorithm_params['max_k']})")
            print(f"Best k Value: {best_k_value}")

            # ç»å…¸ç®—æ³•å¯¹æ¯”è¾“å‡º
            if run_classical and classical_results:
                print(f"\n{'â”€'*50}")
                print(f"Classical Algorithms Comparison")
                print(f"{'â”€'*50}")

                graph_time = time.time() - graph_start_time
                qaoa_time_ms = graph_time * 1000
                qaoa_valid = "Yes" if final_conflicts == 0 else "No"

                print(f"{'Algorithm':<20} {'Colors':<10} {'Conflicts':<10} {'Time (ms)':<15} {'Valid'}")
                print(f"{'â”€'*65}")
                print(f"{'QAOA-Adapt':<20} {unique_colors:<10} {final_conflicts:<10} {qaoa_time_ms:<15.2f} {qaoa_valid}")

                if 'greedy' in classical_results:
                    greedy = classical_results['greedy']
                    greedy_valid = "Yes" if greedy['conflicts'] == 0 else "No"
                    print(f"{'Greedy':<20} {greedy['num_colors']:<10} {greedy['conflicts']:<10} "
                          f"{greedy['execution_time_ms']:<15.2f} {greedy_valid}")

                if 'welch_powell' in classical_results:
                    wp = classical_results['welch_powell']
                    wp_valid = "Yes" if wp['conflicts'] == 0 else "No"
                    print(f"{'Welch-Powell':<20} {wp['num_colors']:<10} {wp['conflicts']:<10} "
                          f"{wp['execution_time_ms']:<15.2f} {wp_valid}")
                print(f"{'â”€'*65}")

            # 7. å­å›¾ç€è‰²å¯è§†åŒ–
            try:
                subgraph_colorings = [
                    get_subgraph_coloring(subgraph, final_coloring, mk)
                    for subgraph, mk in zip(subgraphs, min_k_list)
                ]
                plot_New_IDs_colored_subgraphs(
                    subgraphs, subgraph_colorings, sub_mappings, min_k_list,
                    title=f"[Adapt-QAOA] {base_title} - Colored Subgraphs (Renumbered)",
                    filename=f"{base_title}_colored_subgraphs_renumbered",
                    output_dir=os.path.join(BASE_DIR, "large_subgraph_visualizations")
                )
                plot_Original_IDs_colored_subgraphs(
                    subgraphs, subgraph_colorings,
                    title=f"[Adapt-QAOA] {base_title} - Colored Subgraphs (Original IDs)",
                    min_k_list=min_k_list, filename=f"{base_title}_colored_subgraphs_original",
                    output_dir=os.path.join(BASE_DIR, "large_subgraph_visualizations")
                )
            except Exception as e:
                print(f"Error in colored subgraph plotting for graph {index}: {str(e)}")
                traceback.print_exc()

            # 8. æœ€ç»ˆå›¾å¯è§†åŒ–
            graph_time = time.time() - graph_start_time
            try:
                final_graph_title = (
                    f"[Adapt-QAOA] {base_title}\n"
                    f"Coloring Result (Colors: {unique_colors}, "
                    f"Nodes: {num_nodes}, Edges: {num_edges}, "
                    f"Conflicts: {final_conflicts})"
                )
                visualize_graph(
                    graph, coloring=final_coloring, title=final_graph_title,
                    index=index, min_k=unique_colors,
                    filename=f"adapt_{base_title}_final_coloring",
                    output_dir=os.path.join(BASE_DIR, "large_graph_visualizations"),
                    processing_time=graph_time
                )
            except Exception as e:
                handle_exception("visualize_graph", index, e)

            # 9. æ”¶é›†ç»“æœ
            result = {
                "graph_index": index,
                "graph": graph,
                "final_coloring": final_coloring,
                "subgraphs": subgraphs,
                "sub_mappings": sub_mappings,
                "subgraph_results": subgraph_results,
                "sub_colorings": subgraph_colorings if 'subgraph_colorings' in locals() else [],
                "conflict_counts": conflict_counts,
                "conflict_history": conflict_history,
                "subgraph_opt_history": sub_opt_hist,
                "unique_colors": unique_colors,
                "final_conflicts": final_conflicts,
                "accuracy": opt_acc,
                "processing_time": graph_time,
                "num_nodes": num_nodes,
                "num_edges": num_edges,
                "base_title": base_title,
                "global_max_k": algorithm_params["max_k"],
                "best_k_value": best_k_value,
                "reoptimization_count": reoptimization_count
            }
            all_results.append(result)

            # 10. å†™å…¨å±€æ—¥å¿—
            with open(graph_log_csv, "a", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                conflict_changes_str = ",".join(map(str, conflict_counts)) if conflict_counts else "N/A"
                writer.writerow([
                    dataset_name, graph_name, index,
                    num_nodes, num_edges, final_conflicts, num_edges,
                    round(opt_acc, 4), unique_colors,
                    algorithm_params["max_k"], best_k_value,
                    reoptimization_count, round(graph_time, 4),
                    conflict_changes_str,
                    round(time.time() - total_start_time, 4)
                ])

        except Exception as e:
            print(f"Uncaught exception while processing graph {index}: {e}")
            traceback.print_exc()
            continue

    # ç»“æœæ±‡æ€»
    total_time = time.time() - total_start_time
    print(f"\n{'='*60}")
    print(f"Large-scale Adapt-QAOA all graphs processed, total time: {total_time:.1f}s")
    print(f"Successfully processed {len(all_results)}/{len(graphs)} graphs")
    print(f"CSV: {subgraph_csv} | Logs: {graph_log_csv}")
    print(f"{'='*60}")
    return all_results


def main_adapt_noise_large(graphs, dataset, graph_index, seed, run_classical=False, depolarizing_prob=0.05):
    """
    Noisy Adapt-QAOA å¤§è§„æ¨¡æ•°æ®é›†å…¥å£ï¼ˆåŠ è½½å¤§è§„æ¨¡æ•°æ®ï¼Œä½¿ç”¨æ™ºèƒ½åˆ’åˆ†ï¼‰
    """
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    LOGS_DIR = os.path.join(BASE_DIR, "logs")
    CSV_DIR = os.path.join(BASE_DIR, "csvs")
    os.makedirs(LOGS_DIR, exist_ok=True)
    os.makedirs(CSV_DIR, exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "large_graph_visualizations"), exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "large_subgraph_visualizations"), exist_ok=True)

    # Log file path (includes noise parameter identifier)
    noise_suffix = f"_noise_{depolarizing_prob:.3f}"
    subgraph_csv = os.path.join(CSV_DIR, f"large_datasets_adapt_noise_subgraph_results{noise_suffix}.csv")
    if not os.path.exists(subgraph_csv):
        with open(subgraph_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "subgraph_index",
                "nodes", "edges", "min_k", "conflicts", "status",
                "processing_time", "depolarizing_prob"
            ])

    graph_log_csv = os.path.join(LOGS_DIR, f"large_datasets_adapt_noise_graph_results{noise_suffix}.log")
    if not os.path.exists(graph_log_csv):
        with open(graph_log_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "nodes", "edges",
                "final_conflicts", "total_edges", "final_accuracy",
                "unique_colors", "global_max_k", "best_k_value",
                "subgraph_reoptimization_count", "processing_time",
                "conflict_changes", "total_time", "depolarizing_prob"
            ])

    # åˆ›å»ºç»å…¸ç®—æ³•ç»“æœCSV
    classical_csv = os.path.join(CSV_DIR, f"large_datasets_adapt_noise_classical_results{noise_suffix}.csv")
    if run_classical and not os.path.exists(classical_csv):
        with open(classical_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "algorithm",
                "num_nodes", "num_edges", "num_colors", "conflicts",
                "is_valid", "execution_time_ms"
            ])

    # é…ç½®æ—¥å¿—
    log_file = os.path.join(LOGS_DIR, f"large_adapt_noise_qaoa{noise_suffix}.log")
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logger = logging.getLogger(__name__)

    all_results = []
    total_start_time = time.time()

    # ç®—æ³•å‚æ•°é…ç½® - é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®é›†ä¼˜åŒ–
    algorithm_params = {
        "max_k": 8,
        "p": 3,
        "num_steps": 500,
        "max_iter": 15,
        "early_stop_threshold": 5,
        "penalty": 1000,
        "Q": 20,
        "depolarizing_prob": depolarizing_prob
    }

    for index, graph in enumerate(graphs, start=1):
        graph_start_time = time.time()
        try:
            graph_name = getattr(graph, "file_name", f"graph_{index}")
            base_title = os.path.splitext(graph_name)[0]
            num_nodes = graph.number_of_nodes()
            num_edges = graph.number_of_edges()

            print(f"\n{'='*60}")
            print(f"Processing Graph {index}/{len(graphs)}: {base_title} (Noisy Adapt-QAOA Large, p={depolarizing_prob})")
            print(f"Graph Properties: {num_nodes} Nodes, {num_edges} Edges")
            print(f"{'='*60}")

            # 0. è¿è¡Œç»å…¸ç®—æ³•å¯¹æ¯”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            classical_results = {}
            if run_classical:
                print(f"\n{'â”€'*50}")
                print(f"Running Classical Algorithms Comparison")
                print(f"{'â”€'*50}")

                greedy_res = run_greedy_coloring(graph, graph_name)
                if greedy_res:
                    print(f"âœ“ Greedy Algorithm:")
                    print(f"  Colors: {greedy_res['num_colors']}, Conflicts: {greedy_res['conflicts']}, "
                          f"Time: {greedy_res['execution_time_ms']:.2f}ms")
                    classical_results['greedy'] = greedy_res

                    dataset_name = os.path.basename(getattr(graph, "file_name", "unknown").split(os.sep)[0])
                    with open(classical_csv, "a", newline="", encoding="utf-8") as f:
                        csv.writer(f).writerow([
                            dataset_name, graph_name, index, greedy_res['algorithm'],
                            greedy_res['num_nodes'], greedy_res['num_edges'],
                            greedy_res['num_colors'], greedy_res['conflicts'],
                            greedy_res['is_valid'], greedy_res['execution_time_ms']
                        ])

                wp_res = run_welch_powell_coloring(graph, graph_name)
                if wp_res:
                    print(f"âœ“ Welch-Powell Algorithm:")
                    print(f"  Colors: {wp_res['num_colors']}, Conflicts: {wp_res['conflicts']}, "
                          f"Time: {wp_res['execution_time_ms']:.2f}ms")
                    classical_results['welch_powell'] = wp_res

                    with open(classical_csv, "a", newline="", encoding="utf-8") as f:
                        csv.writer(f).writerow([
                            dataset_name, graph_name, index, wp_res['algorithm'],
                            wp_res['num_nodes'], wp_res['num_edges'],
                            wp_res['num_colors'], wp_res['conflicts'],
                            wp_res['is_valid'], wp_res['execution_time_ms']
                        ])

            # 1. åŸå§‹å›¾å¯è§†åŒ–
            try:
                filename = f"{base_title}_original"
                plot_original_graph(
                    graph,
                    title=f"[Noisy Adapt-QAOA] {base_title} - Original Graph (Nodes: {num_nodes}, Edges: {num_edges}, Noise: {depolarizing_prob})",
                    filename=filename,
                    output_dir=os.path.join(BASE_DIR, "large_graph_visualizations")
                )
            except Exception as e:
                handle_exception("plot_original_graph", index, e)

            # 2. æ™ºèƒ½å­å›¾åˆ’åˆ†ï¼ˆé™åˆ¶é‡å­æ¯”ç‰¹æ•°æœ€å¤šä¸º21ï¼‰
            max_qubits = 21
            subgraphs, sub_mappings, divide_info = smart_divide_graph_with_qubit_constraint(
                graph,
                max_qubits=max_qubits,
                max_k_per_subgraph=algorithm_params["max_k"],
                Q=algorithm_params["Q"]
            )
            logger.info(f"Noisy Adapt-QAOA æ™ºèƒ½åˆ’åˆ†å®Œæˆ: {len(subgraphs)} ä¸ªå­å›¾ï¼ˆé‡å­æ¯”ç‰¹çº¦æŸï¼šâ‰¤{max_qubits}ï¼‰")

            # 3. å­å›¾å¯è§†åŒ–
            try:
                plot_New_IDs_subgraphs(
                    subgraphs, sub_mappings,
                    title=f"[Noisy Adapt-QAOA] {base_title} - Subgraphs (Renumbered)",
                    filename=f"{base_title}_subgraphs_renumbered",
                    output_dir=os.path.join(BASE_DIR, "large_subgraph_visualizations")
                )
                plot_Original_IDs_subgraphs(
                    subgraphs,
                    title=f"[Noisy Adapt-QAOA] {base_title} - Subgraphs (Original IDs)",
                    filename=f"{base_title}_subgraphs_original",
                    output_dir=os.path.join(BASE_DIR, "large_subgraph_visualizations")
                )
            except Exception as e:
                handle_exception("subgraph plotting", index, e)

            # 4. Noisy Adapt-QAOA å­å›¾å¤„ç†
            subgraph_start_time = time.time()
            subgraph_results = sequential_process_subgraphs_noise(
                subgraphs=subgraphs,
                sub_mappings=sub_mappings,
                dataset_name=dataset,
                graph_id=index,
                max_k=algorithm_params["max_k"],
                p=algorithm_params["p"],
                num_steps=algorithm_params["num_steps"],
                vertex_colors=None,
                nodes_to_recolor=None,
                penalty=algorithm_params["penalty"],
                Q=algorithm_params["Q"],
                learning_rate=0.01,
                depolarizing_prob=algorithm_params["depolarizing_prob"]
            )
            subgraph_total_time = time.time() - subgraph_start_time

            # è®°å½•å­å›¾çº§åˆ«ç»“æœ
            dataset_name = os.path.basename(getattr(graph, "file_name", "unknown").split(os.sep)[0])
            for sub_idx, result in enumerate(subgraph_results):
                if result is None:
                    continue
                min_k, coloring, conflicts, status, _ = result
                subgraph = subgraphs[sub_idx] if sub_idx < len(subgraphs) else None
                sub_nodes = subgraph.number_of_nodes() if subgraph else 0
                sub_edges = subgraph.number_of_edges() if subgraph else 0
                with open(subgraph_csv, "a", newline="", encoding="utf-8") as f:
                    csv.writer(f).writerow([
                        dataset_name, graph_name, index, sub_idx + 1,
                        sub_nodes, sub_edges, min_k, conflicts, status,
                        round(subgraph_total_time / len(subgraphs), 4) if subgraphs else 0,
                        depolarizing_prob
                    ])

            # 5. Noisy Adapt-QAOA è¿­ä»£ä¼˜åŒ–
            opt_color, opt_acc, conflict_counts, conflict_history, sub_opt_hist = iterative_optimization_noise(
                graph=graph,
                subgraphs=subgraphs,
                sub_mappings=sub_mappings,
                subgraph_results=subgraph_results,
                max_k=algorithm_params["max_k"],
                p=algorithm_params["p"],
                num_steps=algorithm_params["num_steps"],
                max_iter=algorithm_params["max_iter"],
                adjacency_threshold=0.3,
                early_stop_threshold=algorithm_params["early_stop_threshold"],
                penalty=algorithm_params["penalty"],
                Q=algorithm_params["Q"],
                learning_rate=0.01,
                vertex_colors=None,
                nodes_to_recolor=None,
                dataset_name=dataset,
                graph_id=index,
                depolarizing_prob=algorithm_params["depolarizing_prob"]
            )

            # 6. ç»Ÿè®¡ç»“æœ
            final_coloring = opt_color
            unique_colors = len(set(final_coloring.values())) if final_coloring else 0
            final_conflicts = count_conflicts(final_coloring, graph) if final_coloring else -1
            reoptimization_count = sum(
                1 for h in sub_opt_hist
                if isinstance(h, tuple) and len(h) >= 4 and h[3] > 0
            )
            min_k_list = [r[0] for r in subgraph_results if r is not None and r[0] is not None]
            best_k_value = min(unique_colors, max(min_k_list) if min_k_list else unique_colors)

            print(f"\n===== Optimization Summary (Noisy Adapt-QAOA Large) =====")
            print(f"Final Conflicts: {final_conflicts} (Total Edges: {num_edges})")
            print(f"Final Accuracy: {opt_acc:.4f}")
            print(f"Colors Used: {unique_colors} (Global max_k limit: {algorithm_params['max_k']})")
            print(f"Best k Value: {best_k_value}")
            print(f"Noise Parameter: Depolarizing Probability = {depolarizing_prob}")

            # ç»å…¸ç®—æ³•å¯¹æ¯”è¾“å‡º
            if run_classical and classical_results:
                print(f"\n{'â”€'*50}")
                print(f"Classical Algorithms Comparison")
                print(f"{'â”€'*50}")

                graph_time = time.time() - graph_start_time
                qaoa_time_ms = graph_time * 1000
                qaoa_valid = "Yes" if final_conflicts == 0 else "No"

                print(f"{'Algorithm':<20} {'Colors':<10} {'Conflicts':<10} {'Time (ms)':<15} {'Valid'}")
                print(f"{'â”€'*65}")
                print(f"{'QAOA-Adapt-Noise':<20} {unique_colors:<10} {final_conflicts:<10} {qaoa_time_ms:<15.2f} {qaoa_valid}")

                if 'greedy' in classical_results:
                    greedy = classical_results['greedy']
                    greedy_valid = "Yes" if greedy['conflicts'] == 0 else "No"
                    print(f"{'Greedy':<20} {greedy['num_colors']:<10} {greedy['conflicts']:<10} "
                          f"{greedy['execution_time_ms']:<15.2f} {greedy_valid}")

                if 'welch_powell' in classical_results:
                    wp = classical_results['welch_powell']
                    wp_valid = "Yes" if wp['conflicts'] == 0 else "No"
                    print(f"{'Welch-Powell':<20} {wp['num_colors']:<10} {wp['conflicts']:<10} "
                          f"{wp['execution_time_ms']:<15.2f} {wp_valid}")
                print(f"{'â”€'*65}")

            # 7. å­å›¾ç€è‰²å¯è§†åŒ–
            try:
                subgraph_colorings = [
                    get_subgraph_coloring(subgraph, final_coloring, mk)
                    for subgraph, mk in zip(subgraphs, min_k_list)
                ]
                plot_New_IDs_colored_subgraphs(
                    subgraphs, subgraph_colorings, sub_mappings, min_k_list,
                    title=f"[Noisy Adapt-QAOA] {base_title} - Colored Subgraphs (Renumbered)",
                    filename=f"{base_title}_colored_subgraphs_renumbered",
                    output_dir=os.path.join(BASE_DIR, "large_subgraph_visualizations")
                )
                plot_Original_IDs_colored_subgraphs(
                    subgraphs, subgraph_colorings,
                    title=f"[Noisy Adapt-QAOA] {base_title} - Colored Subgraphs (Original IDs)",
                    min_k_list=min_k_list, filename=f"{base_title}_colored_subgraphs_original",
                    output_dir=os.path.join(BASE_DIR, "large_subgraph_visualizations")
                )
            except Exception as e:
                print(f"Error in colored subgraph plotting for graph {index}: {str(e)}")
                traceback.print_exc()

            # 8. æœ€ç»ˆå›¾å¯è§†åŒ–
            graph_time = time.time() - graph_start_time
            try:
                final_graph_title = (
                    f"[Noisy Adapt-QAOA] {base_title}\n"
                    f"Coloring Result (Colors: {unique_colors}, "
                    f"Nodes: {num_nodes}, Edges: {num_edges}, "
                    f"Conflicts: {final_conflicts}, Noise: {depolarizing_prob})"
                )
                visualize_graph(
                    graph, coloring=final_coloring, title=final_graph_title,
                    index=index, min_k=unique_colors,
                    filename=f"adapt_noise_{base_title}_final_coloring",
                    output_dir=os.path.join(BASE_DIR, "large_graph_visualizations"),
                    processing_time=graph_time
                )
            except Exception as e:
                handle_exception("visualize_graph", index, e)

            # 9. æ”¶é›†ç»“æœ
            result = {
                "graph_index": index,
                "graph": graph,
                "final_coloring": final_coloring,
                "subgraphs": subgraphs,
                "sub_mappings": sub_mappings,
                "subgraph_results": subgraph_results,
                "sub_colorings": subgraph_colorings if 'subgraph_colorings' in locals() else [],
                "conflict_counts": conflict_counts,
                "conflict_history": conflict_history,
                "subgraph_opt_history": sub_opt_hist,
                "unique_colors": unique_colors,
                "final_conflicts": final_conflicts,
                "accuracy": opt_acc,
                "processing_time": graph_time,
                "num_nodes": num_nodes,
                "num_edges": num_edges,
                "base_title": base_title,
                "global_max_k": algorithm_params["max_k"],
                "best_k_value": best_k_value,
                "reoptimization_count": reoptimization_count,
                "noise_params": {"depolarizing_prob": depolarizing_prob}
            }
            all_results.append(result)

            # 10. å†™å…¨å±€æ—¥å¿—
            with open(graph_log_csv, "a", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                conflict_changes_str = ",".join(map(str, conflict_counts)) if conflict_counts else "N/A"
                writer.writerow([
                    dataset_name, graph_name, index,
                    num_nodes, num_edges, final_conflicts, num_edges,
                    round(opt_acc, 4), unique_colors,
                    algorithm_params["max_k"], best_k_value,
                    reoptimization_count, round(graph_time, 4),
                    conflict_changes_str,
                    round(time.time() - total_start_time, 4),
                    depolarizing_prob
                ])

        except Exception as e:
            print(f"Uncaught exception while processing graph {index}: {e}")
            traceback.print_exc()
            continue

    # ç»“æœæ±‡æ€»
    total_time = time.time() - total_start_time
    print(f"\n{'='*60}")
    print(f"Large-scale Noisy Adapt-QAOA all graphs processed, total time: {total_time:.1f}s")
    print(f"Noise Parameter: Depolarizing Probability = {depolarizing_prob}")
    print(f"Successfully processed {len(all_results)}/{len(graphs)} graphs")
    print(f"CSV: {subgraph_csv} | Logs: {graph_log_csv}")
    print(f"{'='*60}")
    return all_results


def main_standard_large(graphs, dataset, graph_index, seed, run_classical=False):
    """
    æ ‡å‡† QAOA å¤§è§„æ¨¡æ•°æ®é›†å…¥å£
    """
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    LOGS_DIR = os.path.join(BASE_DIR, "logs")
    CSV_DIR = os.path.join(BASE_DIR, "csvs")
    os.makedirs(LOGS_DIR, exist_ok=True)
    os.makedirs(CSV_DIR, exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "large_graph_visualizations"), exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "large_subgraph_visualizations"), exist_ok=True)

    subgraph_csv = os.path.join(CSV_DIR, "large_datasets_standard_subgraph_results.csv")
    if not os.path.exists(subgraph_csv):
        with open(subgraph_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "subgraph_index",
                "nodes", "edges", "min_k", "conflicts", "status", "processing_time"
            ])

    graph_log_csv = os.path.join(LOGS_DIR, "large_datasets_standard_graph_results.log")
    if not os.path.exists(graph_log_csv):
        with open(graph_log_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "nodes", "edges",
                "final_conflicts", "total_edges", "final_accuracy",
                "unique_colors", "global_max_k", "best_k_value",
                "subgraph_reoptimization_count", "processing_time",
                "conflict_changes", "total_time"
            ])

    # åˆ›å»ºç»å…¸ç®—æ³•ç»“æœCSV
    classical_csv = os.path.join(CSV_DIR, "large_datasets_standard_classical_results.csv")
    if run_classical and not os.path.exists(classical_csv):
        with open(classical_csv, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow([
                "dataset", "graph_name", "graph_index", "algorithm",
                "num_nodes", "num_edges", "num_colors", "conflicts",
                "is_valid", "execution_time_ms"
            ])

    # é…ç½®æ—¥å¿—
    log_file = os.path.join(LOGS_DIR, "large_standard_qaoa.log")
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logger = logging.getLogger(__name__)

    all_results = []
    total_start_time = time.time()

    # ç®—æ³•å‚æ•°é…ç½® - é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®é›†ä¼˜åŒ–
    algorithm_params = {
        "max_k": 8,  # é™ä½max_kä»¥æé«˜æ•ˆç‡
        "p": 1,
        "num_steps": 500,  # å‡å°‘æ­¥æ•°
        "max_iter": 15,
        "early_stop_threshold": 5,
        "penalty": 1000,
        "Q": 20
    }

    # å¤„ç†æ¯å¼ å›¾
    for index, graph in enumerate(graphs, start=1):
        graph_start_time = time.time()
        try:
            graph_name = getattr(graph, "file_name", f"graph_{index}")
            base_title = os.path.splitext(graph_name)[0]
            num_nodes = graph.number_of_nodes()
            num_edges = graph.number_of_edges()

            print(f"\n{'='*60}")
            print(f"Processing Graph {index}/{len(graphs)}: {base_title} (Large-Scale)")
            print(f"Graph Properties: {num_nodes} Nodes, {num_edges} Edges")
            print(f"{'='*60}")

            # 0. è¿è¡Œç»å…¸ç®—æ³•å¯¹æ¯”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            classical_results = {}
            if run_classical:
                print(f"\n{'â”€'*50}")
                print(f"Running Classical Algorithms Comparison")
                print(f"{'â”€'*50}")

                # è´ªå¿ƒç®—æ³•
                greedy_res = run_greedy_coloring(graph, graph_name)
                if greedy_res:
                    print(f"âœ“ Greedy Algorithm:")
                    print(f"  Colors: {greedy_res['num_colors']}, Conflicts: {greedy_res['conflicts']}, "
                          f"Time: {greedy_res['execution_time_ms']:.2f}ms")
                    classical_results['greedy'] = greedy_res

                    # ä¿å­˜åˆ°CSV
                    dataset_name = os.path.basename(getattr(graph, "file_name", "unknown").split(os.sep)[0])
                    with open(classical_csv, "a", newline="", encoding="utf-8") as f:
                        csv.writer(f).writerow([
                            dataset_name, graph_name, index, greedy_res['algorithm'],
                            greedy_res['num_nodes'], greedy_res['num_edges'],
                            greedy_res['num_colors'], greedy_res['conflicts'],
                            greedy_res['is_valid'], greedy_res['execution_time_ms']
                        ])

                # Welch-Powellç®—æ³•
                wp_res = run_welch_powell_coloring(graph, graph_name)
                if wp_res:
                    print(f"âœ“ Welch-Powell Algorithm:")
                    print(f"  Colors: {wp_res['num_colors']}, Conflicts: {wp_res['conflicts']}, "
                          f"Time: {wp_res['execution_time_ms']:.2f}ms")
                    classical_results['welch_powell'] = wp_res

                    # ä¿å­˜åˆ°CSV
                    with open(classical_csv, "a", newline="", encoding="utf-8") as f:
                        csv.writer(f).writerow([
                            dataset_name, graph_name, index, wp_res['algorithm'],
                            wp_res['num_nodes'], wp_res['num_edges'],
                            wp_res['num_colors'], wp_res['conflicts'],
                            wp_res['is_valid'], wp_res['execution_time_ms']
                        ])

            # 1. åŸå§‹å›¾å¯è§†åŒ–
            try:
                filename = f"{base_title}_original"
                plot_original_graph(
                    graph,
                    title=f"{base_title} - Original Graph (Nodes: {num_nodes}, Edges: {num_edges})",
                    filename=filename,
                    output_dir=os.path.join(BASE_DIR, "large_graph_visualizations")
                )
            except Exception as e:
                handle_exception("plot_original_graph", index, e)

            # 2. æ™ºèƒ½å­å›¾åˆ’åˆ†ï¼ˆé™åˆ¶é‡å­æ¯”ç‰¹æ•°æœ€å¤šä¸º21ï¼‰
            max_qubits = 21
            subgraphs, sub_mappings, divide_info = smart_divide_graph_with_qubit_constraint(
                graph,
                max_qubits=max_qubits,
                max_k_per_subgraph=algorithm_params["max_k"],
                Q=algorithm_params["Q"]
            )
            logger.info(f"æ™ºèƒ½åˆ’åˆ†å®Œæˆ: {len(subgraphs)} ä¸ªå­å›¾ï¼ˆé‡å­æ¯”ç‰¹çº¦æŸï¼šâ‰¤{max_qubits}ï¼‰")

            # 3. å­å›¾å¯è§†åŒ–
            try:
                plot_New_IDs_subgraphs(
                    subgraphs, sub_mappings,
                    title=f"{base_title} - Subgraphs (Renumbered)",
                    filename=f"{base_title}_subgraphs_renumbered",
                    output_dir=os.path.join(BASE_DIR, "large_subgraph_visualizations")
                )
                plot_Original_IDs_subgraphs(
                    subgraphs,
                    title=f"{base_title} - Subgraphs (Original IDs)",
                    filename=f"{base_title}_subgraphs_original",
                    output_dir=os.path.join(BASE_DIR, "large_subgraph_visualizations")
                )
            except Exception as e:
                handle_exception("subgraph plotting", index, e)

            # 4. æ ‡å‡† QAOA å­å›¾å¤„ç†
            subgraph_start_time = time.time()
            subgraph_results = sequential_process_subgraphs_standard(
                subgraphs=subgraphs,
                sub_mappings=sub_mappings,
                dataset_name=dataset,
                graph_id=index,
                max_k=algorithm_params["max_k"],
                p=algorithm_params["p"],
                num_steps=algorithm_params["num_steps"],
                vertex_colors=None,
                nodes_to_recolor=None,
                penalty=algorithm_params["penalty"],
                Q=algorithm_params["Q"],
                learning_rate=0.01
            )
            subgraph_total_time = time.time() - subgraph_start_time

            # è®°å½•å­å›¾çº§åˆ«ç»“æœ
            dataset_name = os.path.basename(getattr(graph, "file_name", "unknown").split(os.sep)[0])
            for sub_idx, result in enumerate(subgraph_results):
                if result is None:
                    continue
                min_k, coloring, conflicts, status, _ = result
                subgraph = subgraphs[sub_idx] if sub_idx < len(subgraphs) else None
                sub_nodes = subgraph.number_of_nodes() if subgraph else 0
                sub_edges = subgraph.number_of_edges() if subgraph else 0
                with open(subgraph_csv, "a", newline="", encoding="utf-8") as f:
                    csv.writer(f).writerow([
                        dataset_name, graph_name, index, sub_idx + 1,
                        sub_nodes, sub_edges, min_k, conflicts, status,
                        round(subgraph_total_time / len(subgraphs), 4) if subgraphs else 0
                    ])

            # 5. æ ‡å‡† QAOA è¿­ä»£ä¼˜åŒ–
            optimized_coloring, opt_acc, conflict_counts, conflict_history, subgraph_opt_history = iterative_optimization_standard(
                graph=graph,
                subgraphs=subgraphs,
                sub_mappings=sub_mappings,
                subgraph_results=subgraph_results,
                max_k=algorithm_params["max_k"],
                p=algorithm_params["p"],
                num_steps=algorithm_params["num_steps"],
                max_iter=algorithm_params["max_iter"],
                adjacency_threshold=0.3,
                early_stop_threshold=algorithm_params["early_stop_threshold"],
                penalty=algorithm_params["penalty"],
                Q=algorithm_params["Q"],
                learning_rate=0.01,
                vertex_colors=None,
                nodes_to_recolor=None,
                dataset_name=dataset,
                graph_id=index
            )

            # 6. ç»Ÿè®¡ç»“æœ
            final_coloring = optimized_coloring
            unique_colors = len(set(final_coloring.values())) if final_coloring else 0
            final_conflicts = count_conflicts(final_coloring, graph) if final_coloring else -1
            reoptimization_count = sum(
                1 for h in subgraph_opt_history
                if isinstance(h, tuple) and len(h) >= 4 and h[3] > 0
            )
            min_k_list = [r[0] for r in subgraph_results if r is not None and r[0] is not None]
            best_k_value = min(unique_colors, max(min_k_list) if min_k_list else unique_colors)

            print(f"\n===== Optimization Summary (Standard-QAOA Large) =====")
            print(f"Final Conflicts: {final_conflicts} (Total Edges: {num_edges})")
            print(f"Final Accuracy: {opt_acc:.4f}")
            print(f"Colors Used: {unique_colors} (Global max_k limit: {algorithm_params['max_k']})")
            print(f"Best k Value: {best_k_value}")

            # ç»å…¸ç®—æ³•å¯¹æ¯”è¾“å‡º
            if run_classical and classical_results:
                print(f"\n{'â”€'*50}")
                print(f"Classical Algorithms Comparison")
                print(f"{'â”€'*50}")

                graph_time = time.time() - graph_start_time
                qaoa_time_ms = graph_time * 1000
                qaoa_valid = "Yes" if final_conflicts == 0 else "No"

                print(f"{'Algorithm':<20} {'Colors':<10} {'Conflicts':<10} {'Time (ms)':<15} {'Valid'}")
                print(f"{'â”€'*65}")
                print(f"{'QAOA-Standard':<20} {unique_colors:<10} {final_conflicts:<10} {qaoa_time_ms:<15.2f} {qaoa_valid}")

                if 'greedy' in classical_results:
                    greedy = classical_results['greedy']
                    greedy_valid = "Yes" if greedy['conflicts'] == 0 else "No"
                    print(f"{'Greedy':<20} {greedy['num_colors']:<10} {greedy['conflicts']:<10} "
                          f"{greedy['execution_time_ms']:<15.2f} {greedy_valid}")

                if 'welch_powell' in classical_results:
                    wp = classical_results['welch_powell']
                    wp_valid = "Yes" if wp['conflicts'] == 0 else "No"
                    print(f"{'Welch-Powell':<20} {wp['num_colors']:<10} {wp['conflicts']:<10} "
                          f"{wp['execution_time_ms']:<15.2f} {wp_valid}")
                print(f"{'â”€'*65}")

            # 7. å­å›¾ç€è‰²å¯è§†åŒ–
            try:
                subgraph_colorings = [
                    get_subgraph_coloring(subgraph, final_coloring, mk)
                    for subgraph, mk in zip(subgraphs, min_k_list)
                ]
                plot_New_IDs_colored_subgraphs(
                    subgraphs, subgraph_colorings, sub_mappings, min_k_list,
                    title=f"{base_title} - Colored Subgraphs (Renumbered)",
                    filename=f"{base_title}_colored_subgraphs_renumbered",
                    output_dir=os.path.join(BASE_DIR, "large_subgraph_visualizations")
                )
                plot_Original_IDs_colored_subgraphs(
                    subgraphs, subgraph_colorings,
                    title=f"{base_title} - Colored Subgraphs (Original IDs)",
                    min_k_list=min_k_list, filename=f"{base_title}_colored_subgraphs_original",
                    output_dir=os.path.join(BASE_DIR, "large_subgraph_visualizations")
                )
            except Exception as e:
                print(f"Error in colored subgraph plotting for graph {index}: {str(e)}")
                traceback.print_exc()

            # 8. æœ€ç»ˆå›¾å¯è§†åŒ–
            graph_time = time.time() - graph_start_time
            try:
                final_graph_title = (
                    f"{base_title}\n"
                    f"Coloring Result (Colors: {unique_colors}, "
                    f"Nodes: {num_nodes}, Edges: {num_edges}, "
                    f"Conflicts: {final_conflicts})"
                )
                visualize_graph(
                    graph, coloring=final_coloring, title=final_graph_title,
                    index=index, min_k=unique_colors,
                    filename=f"{base_title}_final_coloring",
                    output_dir=os.path.join(BASE_DIR, "large_graph_visualizations"),
                    processing_time=graph_time
                )
            except Exception as e:
                handle_exception("visualize_graph", index, e)

            # 9. æ”¶é›†ç»“æœ
            result = {
                "graph_index": index,
                "graph": graph,
                "final_coloring": final_coloring,
                "subgraphs": subgraphs,
                "sub_mappings": sub_mappings,
                "subgraph_results": subgraph_results,
                "sub_colorings": subgraph_colorings if 'subgraph_colorings' in locals() else [],
                "conflict_counts": conflict_counts,
                "conflict_history": conflict_history,
                "subgraph_opt_history": subgraph_opt_history,
                "unique_colors": unique_colors,
                "final_conflicts": final_conflicts,
                "accuracy": opt_acc,
                "processing_time": graph_time,
                "num_nodes": num_nodes,
                "num_edges": num_edges,
                "base_title": base_title,
                "global_max_k": algorithm_params["max_k"],
                "best_k_value": best_k_value,
                "reoptimization_count": reoptimization_count
            }
            all_results.append(result)

            # 10. å†™å…¨å±€æ—¥å¿—
            with open(graph_log_csv, "a", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                conflict_changes_str = ",".join(map(str, conflict_counts)) if conflict_counts else "N/A"
                writer.writerow([
                    dataset_name, graph_name, index,
                    num_nodes, num_edges, final_conflicts, num_edges,
                    round(opt_acc, 4), unique_colors,
                    algorithm_params["max_k"], best_k_value,
                    reoptimization_count, round(graph_time, 4),
                    conflict_changes_str,
                    round(time.time() - total_start_time, 4)
                ])

        except Exception as e:
            print(f"Uncaught exception while processing graph {index}: {e}")
            traceback.print_exc()
            continue

    # ç»“æœæ±‡æ€»
    total_time = time.time() - total_start_time
    print(f"\n{'='*60}")
    print(f"Large-scale Standard-QAOA all graphs processed, total time: {total_time:.1f}s")
    print(f"Successfully processed {len(all_results)}/{len(graphs)} graphs")
    print(f"CSV: {subgraph_csv} | Logs: {graph_log_csv}")
    print(f"{'='*60}")
    return all_results


def main_large_datasets():
    """
    ä¸»å‡½æ•°ï¼šå¤„ç†å¤§è§„æ¨¡æ•°æ®é›† (cora, citeseer, pubmed)
    """
    args = parse_test_args()

    # åŠ è½½å¤§è§„æ¨¡æ•°æ®é›†
    graphs = load_large_datasets()

    if not graphs:
        print("\nâš ï¸ æœªèƒ½åŠ è½½ä»»ä½•å›¾æ•°æ®ï¼Œç¨‹åºé€€å‡º")
        return

    print(f"\nâœ“ æˆåŠŸåŠ è½½ {len(graphs)} å¼ å¤§è§„æ¨¡å›¾")

    # æ‰“å°æ•°æ®åˆ†æ
    MAX_NODES_PER_SUBGRAPH = 50  # æ ¹æ®ç¡¬ä»¶é™åˆ¶è°ƒæ•´
    print_large_dataset_analysis(graphs, MAX_NODES_PER_SUBGRAPH)

    SEED = args.seed
    DATASET = "large_datasets"
    RUN_CLASSICAL = args.run_classical

    # éªŒè¯è‡³å°‘é€‰æ‹©ä¸€ç§ç®—æ³•
    if not any([args.adapt, args.standard, args.adapt_noise]):
        print("âš ï¸ Must select at least one algorithm (--adapt/--standard/--adapt-noise)")
        print("Usage examples for large datasets:")
        print("  python Main_Multilevel_qaoa_large_graph.py --large-datasets --standard")
        print("  python Main_Multilevel_qaoa_large_graph.py --large-datasets --standard --run-classical")
        return

    results = {}

    # è¿è¡Œ Adapt-QAOA
    if args.adapt:
        print("\n" + "="*60)
        print("Starting Adapt-QAOA on Large Datasets...")
        print("="*60)
        try:
            adapt_results = main_adapt_large(
                graphs=graphs,
                dataset=DATASET,
                graph_index=0,
                seed=SEED,
                run_classical=RUN_CLASSICAL
            )
            results['adapt'] = adapt_results
            print(f"\nâœ… Adapt-QAOA completed, processed {len(adapt_results)} graphs")
        except Exception as e:
            print(f"\nâš ï¸ Adapt-QAOA failed: {e}")
            traceback.print_exc()

    # è¿è¡Œ Standard-QAOA
    if args.standard:
        print("\n" + "="*60)
        print("Starting Standard-QAOA on Large Datasets...")
        print("="*60)
        try:
            standard_results = main_standard_large(
                graphs=graphs,
                dataset=DATASET,
                graph_index=0,
                seed=SEED,
                run_classical=RUN_CLASSICAL
            )
            results['standard'] = standard_results
            print(f"\nâœ… Standard-QAOA completed, processed {len(standard_results)} graphs")
        except Exception as e:
            print(f"\nâš ï¸ Standard-QAOA failed: {e}")
            traceback.print_exc()

    # è¿è¡Œ Noisy Adapt-QAOA
    if args.adapt_noise:
        print("\n" + "="*60)
        print(f"Starting Noisy Adapt-QAOA on Large Datasets (noise: {args.noise_prob})...")
        print("="*60)
        try:
            noise_results = main_adapt_noise_large(
                graphs=graphs,
                dataset=DATASET,
                graph_index=0,
                seed=SEED,
                run_classical=RUN_CLASSICAL,
                depolarizing_prob=args.noise_prob
            )
            results['adapt_noise'] = noise_results
            print(f"\nâœ… Noisy Adapt-QAOA completed, processed {len(noise_results)} graphs")
        except Exception as e:
            print(f"\nâš ï¸ Noisy Adapt-QAOA failed: {e}")
            traceback.print_exc()

    # è¾“å‡ºç»“æœæ±‡æ€»
    print("\n" + "="*70)
    print("                    Large Dataset Results Summary")
    print("="*70)

    for algo_name, algo_results in results.items():
        if not algo_results:
            continue

        print(f"\nã€{algo_name.upper()}ã€‘")
        print("-" * 70)
        print(f"  Successfully Processed Graphs: {len(algo_results)}")
        print(f"  {'Graph Name':<20} {'Nodes':<8} {'Colors':<8} {'Conflicts':<10} {'Accuracy':<12} {'Time(s)':<10}")
        print("  " + "-" * 70)

        for r in algo_results:
            graph_name = os.path.splitext(r.get('base_title', 'unknown'))[0]
            nodes = r['num_nodes']
            colors = r['unique_colors']
            conflicts = r['final_conflicts']
            accuracy = r['accuracy']
            time_cost = r['processing_time']
            print(f"  {graph_name:<20} {nodes:<8} {colors:<8} {conflicts:<10} {accuracy:<12.4f} {time_cost:<10.2f}")

        # è®¡ç®—ç»Ÿè®¡
        avg_colors = sum(r['unique_colors'] for r in algo_results) / len(algo_results)
        avg_time = sum(r['processing_time'] for r in algo_results) / len(algo_results)
        avg_accuracy = sum(r['accuracy'] for r in algo_results) / len(algo_results)
        total_conflicts = sum(r['final_conflicts'] for r in algo_results)

        print("  " + "-" * 70)
        print(f"  Average Colors: {avg_colors:.2f}")
        print(f"  Average Time: {avg_time:.2f} s")
        print(f"  Average Accuracy: {avg_accuracy:.4f}")
        print(f"  Total Conflicts: {total_conflicts}")

    print("\n" + "="*70)
    print("âœ… Large dataset experiments completed!")
    print("="*70 + "\n")



# ============================================================================
# è¯´æ˜ï¼šæœ¬æ–‡ä»¶å·²è¢«ç®€åŒ–ï¼Œä¸»å‡½æ•°å·²ç§»é™¤
# ============================================================================
#
# æœ¬æ–‡ä»¶åŒ…å«å¤§è§„æ¨¡æ•°æ®é›†å¤„ç†å‡½æ•°ï¼Œä½†ä¸»å‡½æ•°å…¥å£å·²ç§»è‡³ Main_Multilevel_qaoa.py
# å¦‚éœ€è¿è¡Œå¤§è§„æ¨¡æ•°æ®é›†å®éªŒï¼Œè¯·ä½¿ç”¨ run_experiments.py æˆ–å‚è€ƒä»¥ä¸‹å‘½ä»¤ï¼š
#
# Adapt-QAOA å¤§è§„æ¨¡æ•°æ®é›†
# python Main_Multilevel_qaoa_large_graph.py --large-datasets --adapt
#
# Standard-QAOA å¤§è§„æ¨¡æ•°æ®é›†
# python Main_Multilevel_qaoa_large_graph.py --large-datasets --standard
#
# Noisy Adapt-QAOA å¤§è§„æ¨¡æ•°æ®é›†
# python Main_Multilevel_qaoa_large_graph.py --large-datasets --adapt-noise --noise-prob 0.05
#
# å¸¦ç»å…¸ç®—æ³•å¯¹æ¯”
# python Main_Multilevel_qaoa_large_graph.py --large-datasets --standard --run-classical
#
# ============================================================================




'''